{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5c8c772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import html\n",
    "import hashlib\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import base64 # to extract images from base64 strings (as they are stored in xml files)\n",
    "from datetime import datetime\n",
    "import html2text\n",
    "import cleanhtml as ch #my own helper functions\n",
    "from loadcalculations import loadcalc\n",
    "import odk_helpers as oh\n",
    "from combinecalculates import calcombo\n",
    "from treetodataframe import treetodataframe\n",
    "import graphtools as gt\n",
    "import qualitychecks_pd as qcpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c5ffad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters (defined in the merge script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78e8a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r form_id testing multiple_labels drugsfile cafile inputfile_dx inputfile_tt \\\n",
    "dxfile ttfile output form_title diagnose_order input_trans updated_trans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "883e3caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters from file -> eventually move everything into this file\n",
    "#%% Parameters\n",
    "# import params as p # for almanach Somalia\n",
    "if form_id == 'ped':\n",
    "    import params_ped as p # for msfecare Ped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf4c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5830b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db727e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634c9059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ca47f42",
   "metadata": {},
   "source": [
    "### Parsing draw.io file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70177962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page ID: C5RBs43oDa-KdzZeNtuy Page name: Diagnostic\n",
      "Page ID: ylmB6omWLCRXDUskumTV Page name: Skin\n",
      "Page ID: XpsJug1Js2Xvaj5lXfPK Page name: Anaphylaxis\n",
      "Page ID: lYY_49SUh6R5CqtrfGcX Page name: Tests\n",
      "Page ID: wzLc2dvL2DyrwwTwhVoo Page name: Consultant feedback\n"
     ]
    }
   ],
   "source": [
    "data = etree.parse(inputfile_dx) # 'data' is a wrapper for the entire tree\n",
    "root = data.getroot() # get the name of the highest element of the tree, put it into the variable 'root'\n",
    "pages = root.findall('.//diagram') # gets all the tabs of the document\n",
    "\n",
    "objects = [] # all objects of all pages combined\n",
    "\n",
    "for page in pages:\n",
    "    print('Page ID:', page.attrib['id'], 'Page name:', page.attrib['name'])\n",
    "    objects_in_page = page.findall('.//mxGraphModel//mxCell')\n",
    "    objects = objects + objects_in_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6b0bffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = treetodataframe(objects)\n",
    "\n",
    "# maintain compatibility with old script:\n",
    "df_raw.fillna('', inplace = True)\n",
    "df_raw['tag']=''\n",
    "df_raw.loc[df_raw['label']!='','value'] = df_raw['label']\n",
    "df_raw['label_userObject']=''\n",
    "df_raw['xml-parent']=df_raw['parent']\n",
    "\n",
    "df_raw = df_raw[['tag', 'id', 'value', 'label_userObject', 'style', 'xml-parent',\n",
    "       'source', 'target', 'name', 'odk_type', 'min', 'max', 'required',\n",
    "       'constraint_message', 'x', 'y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1486a85f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473ac592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db676e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dba46ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba6487d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cd0016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbc9eee5",
   "metadata": {},
   "source": [
    "### Quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "842ee9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#qcpd.check_node_type(df_raw) # check if all objects have an odk_type\n",
    "#qcpd.check_rhombus_refer(df_raw) # check if all rhombus refer to an existing node\n",
    "#qcpd.check_edge_connection(df_raw) # check if all edges are well connected\n",
    "#types = ['rhombus', 'select_one yesno']\n",
    "#qcpd.check_edge_yesno(df_raw, types) # check if all edges leaving rhombus and select_one yesno have Yes/No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4587ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522093a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10fdd45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4755f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdc57ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify break points for the PAUSE function (colored green)\n",
    "df_pause = df_raw.loc[df_raw['style'].str.contains('fillColor=#cdeb8b', na=False),['id', 'name', 'odk_type']]\n",
    "df_pause['flowtype'] = 'diagnostic'\n",
    "df_pause.to_csv('breakpoints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a416084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a constraint column\n",
    "df=df_raw.copy()\n",
    "df.drop(columns=['x','y'],inplace=True)\n",
    "df['constraint']=''\n",
    "df.loc[df['min']!='','constraint']='.>=' + df['min']\n",
    "df.loc[df['max']!='','constraint']=df['constraint'] + ' and .<=' + df['max']\n",
    "df.drop(columns=['min','max'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f135d03",
   "metadata": {},
   "source": [
    "### Required fields\n",
    "if integers and decimals are not REQUIRED, the expression towards the downstream fields must be removed. See below under **Expression for each row**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3df2c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['required']=='yes','required']='true()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee720f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unescape converts codings like &lt into <. \n",
    "# in the xml file html needs to be encoded like that, otherwise it would interfere with the coding of the xml file\n",
    "\n",
    "# that solution is actually working best. From the answer of 'FrBrGeorge'\n",
    "# https://stackoverflow.com/questions/14694482/converting-html-to-text-with-python\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class HTMLFilter(HTMLParser):\n",
    "    text = \"\"\n",
    "    def handle_data(self, data):\n",
    "        self.text += data\n",
    "\n",
    "def html2plain(data): \n",
    "    f = HTMLFilter()\n",
    "    f.feed(data)\n",
    "    return f.text\n",
    "\n",
    "# the soup.text strips off the html formatting also\n",
    "def remove_html(string):\n",
    "    text = html2text.html2text(string) # retrieve pure text from html\n",
    "    text = text.strip('\\n') # get rid of empty lines at the end (and beginning)\n",
    "    text = text.split('\\n') # split string into a list at new lines\n",
    "    text = '\\n'.join([i.strip(' ') for i in text if i]) # in each element in that list strip empty space (at the end of line) \n",
    "    text = text.replace('\\n',' ')\n",
    "    # and delete empty lines\n",
    "    return text\n",
    "\n",
    "def remove_html_value(string):\n",
    "    text = string.strip('\\n') # get rid of empty lines at the end (and beginning)\n",
    "    text = text.split('\\n') # split string into a list at new lines\n",
    "    text = '\\n'.join([i.strip(' ') for i in text if i]) # in each element in that list strip empty space (at the end of line) \n",
    "    # and delete empty lines\n",
    "    return text\n",
    "\n",
    "# remove html formatting and keep text inside rhombus\n",
    "m = df['odk_type']=='rhombus'\n",
    "df.loc[m,'value'] = df.loc[m,'value'].apply(lambda x: remove_html(x) if x!=None else None)\n",
    "#df.loc[m,'value'] = df.loc[m,'value'].replace('\\n',' ',regex=True)\n",
    "\n",
    "# remove html formatting in questions and select_options (not allowed here)\n",
    "# df.loc[df['odk_type'].str.contains('select_',na=False),'value'] = df.loc[df['odk_type'].str.contains('select_',na=False),'value'].apply(lambda x: remove_html(x) if x!=None else None)\n",
    "# df.loc[df['odk_type']=='hint-message','value'] = df.loc[df['odk_type']=='hint-message','value'].apply(lambda x: remove_html(x) if \n",
    "m = ~df['odk_type'].isin(['note','help-message'])\n",
    "df.loc[m,'value'] = df.loc[m,'value'].apply(lambda x: html2plain(x) if x!=None else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2825e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac5438b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22d7dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb81b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "277531d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add id to name of objects with duplicate names, except for calculate, diagnosis, select_option, rhombus, shortcuts, container-hint-media and edges\n",
    "df.loc[df.duplicated(subset=['name'],keep='first') \\\n",
    "       & ~df['odk_type'].isin(['calculate', 'diagnosis', 'select_option', 'rhombus', 'goto', 'container_hint_media', 'help-message', 'hint-message']) \\\n",
    "       & ~df['style'].str.contains('jettySize', na=False), 'name'] = df['name']+df['id']\n",
    "\n",
    "\n",
    "#df.loc[df.duplicated(subset=['name'],keep='first') & ~df['name'].str.contains('opt_',na=False) \\\n",
    "#       & ~df['name'].str.contains('stored_',na=False) \\\n",
    "#       & ~df['name'].str.contains('shortcut_',na=False), 'name']=df['name']+df['id']\n",
    "\n",
    "df.set_index('id',inplace=True)\n",
    "\n",
    "df['Number of outgoing arrows']=df['source'].value_counts()\n",
    "df['Number of incoming arrows']=df['target'].value_counts()\n",
    "df['Number of outgoing arrows'].fillna(0,inplace=True)\n",
    "df['Number of incoming arrows'].fillna(0,inplace=True)\n",
    "\n",
    "# replace NaN with empty strings\n",
    "df.value.fillna('',inplace=True)\n",
    "\n",
    "# deal with duplicates\n",
    "#c=0\n",
    "#for index, elem in df.loc[df['name'].duplicated()].iterrows():\n",
    "#    df.loc[index,'name'] = df.loc[index,'name']+str(c)\n",
    "#    c+=1    \n",
    "\n",
    "# df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e01726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe with connectors only\n",
    "#df_arrows=df_raw.loc[(df_raw['source']!='') & (df_raw['target']!=''),['source','target','value']]\n",
    "df_arrows=df.loc[(df['source']!='') & (df['target']!=''),['source','target','value']]\n",
    "#df_arrows=df.loc[df.source.notna() & df.target.notna(),['source','target','value']]\n",
    "\n",
    "# remove html from the text on the arrows\n",
    "df_arrows.loc[:,'value'] = df_arrows.loc[:,'value'].apply(lambda x: remove_html(x) if x!=None else None)\n",
    "\n",
    "# drop arrows from df\n",
    "df.drop(df_arrows.index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d83e462",
   "metadata": {},
   "source": [
    "### take into account shortcuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "566bb431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take into account shortcuts\n",
    "dfa = df_raw.loc[df_raw['odk_type']=='goto'].copy() # extract shortcut elements and put in a new dataframe\n",
    "dfa.loc[dfa['odk_type']=='goto','name'] = dfa.loc[dfa['odk_type']=='goto','name'].str[9:] # remove prefix\n",
    "# merge with raw-data to get the id of the exit element\n",
    "dfa = dfa.reset_index().merge(df_raw.reset_index()[['id','name']],how = 'left', on='name') \n",
    "exitmap = dict(zip(dfa['id_x'],dfa['id_y'])) # convert into a dictionnary \n",
    "df_arrows['target'] = df_arrows['target'].replace(exitmap) # replace the shortcut elements by the exit-element in df_arrows\n",
    "df.drop(df.loc[df['odk_type']=='goto'].index,inplace=True) # drop shortcuts from df_survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "647a30ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a folder for images and other media\n",
    "\n",
    "if not(os.path.isdir('media')): # check if it exists, because if it does, error will be raised \n",
    "    # (later change to make folder complaint to CHT)\n",
    "    os.mkdir('media')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55c78635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding png images that belong to container-hint-media (not included are those that belong to select_options)\n",
    "df.loc[df['style'].str.contains(\"image/png\",na=False),'odk_type']='png-image'+df.name+'.png'\n",
    "\n",
    "# getting a dataframe with png-images only (better for joining with df later)\n",
    "# images:rows where 'xml-parent' is inside the index of rows that have the entry 'container_hint_media' in odk_type column, \n",
    "# of those rows we extract those where the 'type' column contains the substring 'png-image'\n",
    "# and of the result we just take the columns 'xml-parent', 'odk_type' and 'style'\n",
    "# 'xml-parent' is the container it belongs to and the line that will contain the info about the image\n",
    "# 'odk_type' contains also the file name .png\n",
    "# 'style' contains the actual image data\n",
    "\n",
    "df_png=df.loc[df['xml-parent'].isin(df.loc[df['odk_type']=='container_hint_media'].index) \n",
    "              & df['odk_type'].str.contains('png-image',na=False),\n",
    "              ['xml-parent','odk_type','style']] # images that are in 'containers_hint_media'\n",
    "\n",
    "# getting image data from 'style' column for all images (from containers AND select_options) and storing it to disk\n",
    "df_pngAll=df.loc[df['odk_type'].str.contains('png-image',na=False),['xml-parent','odk_type','style']]\n",
    "for index, row in df_pngAll.iterrows():\n",
    "    string = row['style'] \n",
    "    img_data=re.search('image/png,(.+?);',string).group(1) # extract image data from 'style' column using regex\n",
    "    with open('media/'+row['odk_type'], \"wb\") as fh:\n",
    "        fh.write(base64.decodebytes(img_data.encode('ascii'))) # encode image into ascii (binary) and save\n",
    "\n",
    "df_png.rename({'xml-parent':'container_id','odk_type':'image::en'},axis=1,inplace=True)\n",
    "index_delete=df_png.index\n",
    "df_png.set_index('container_id',inplace=True)\n",
    "df_png.drop('style',axis=1,inplace=True)\n",
    "\n",
    "# joinging df and df_png (this adds the media-image column to df)\n",
    "df=df.join(df_png)\n",
    "\n",
    "# remove the rows with those 'png messages' in df as they are no longer needed\n",
    "df.drop(index_delete,inplace=True)\n",
    "\n",
    "# df.loc[df['image::en'].notna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44bb8ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding jpeg images that belong to container-hint-media (not included are those that belong to select_options)\n",
    "df.loc[df['style'].str.contains(\"image/jpeg\",na=False),'odk_type']='jpeg-image'+df.name+'.jpeg'\n",
    "\n",
    "# getting a dataframe with png-images only (better for joining with df later)\n",
    "# images:rows where 'xml-parent' is inside the index of rows that have the entry 'container_hint_media' in odk_type column, \n",
    "# of those rows we extract those where the 'type' column contains the substring 'png-image'\n",
    "# and of the result we just take the columns 'xml-parent', 'odk_type' and 'style'\n",
    "# 'xml-parent' is the container it belongs to and the line that will contain the info about the image\n",
    "# 'odk_type' contains also the file name .png\n",
    "# 'style' contains the actual image data\n",
    "\n",
    "df_png=df.loc[df['xml-parent'].isin(df.loc[df['odk_type']=='container_hint_media'].index) \n",
    "              & df['odk_type'].str.contains('jpeg-image',na=False),\n",
    "              ['xml-parent','odk_type','style']] # images that are in 'containers_hint_media'\n",
    "\n",
    "# getting image data from 'style' column for all images (from containers AND select_options) and storing it to disk\n",
    "df_pngAll=df.loc[df['odk_type'].str.contains('jpeg-image',na=False),['xml-parent','odk_type','style']]\n",
    "for index, row in df_pngAll.iterrows():\n",
    "    string = row['style'] \n",
    "    img_data=re.search('image/jpeg,(.+?);',string).group(1) # extract image data from 'style' column using regex\n",
    "    with open('media/'+row['odk_type'], \"wb\") as fh:\n",
    "        fh.write(base64.decodebytes(img_data.encode('ascii'))) # encode image into ascii (binary) and save\n",
    "\n",
    "df_png.rename({'xml-parent':'container_id','odk_type':'image::en'},axis=1,inplace=True)\n",
    "index_delete=df_png.index\n",
    "df_png.set_index('container_id',inplace=True)\n",
    "df_png.drop('style',axis=1,inplace=True)\n",
    "\n",
    "# joinging df and df_png (this adds the media-image column to df)\n",
    "#df=df.join(df_png)\n",
    "df.update(df_png)\n",
    "\n",
    "# remove the rows with those 'png messages' in df as they are no longer needed\n",
    "df.drop(index_delete,inplace=True)\n",
    "\n",
    "# df.loc[df['image::en'].notna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bbbe44",
   "metadata": {},
   "source": [
    "### Create and populate 'help' & 'hint' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54218af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in ['hint-message', 'help-message']:\n",
    "\n",
    "    dfa=df_raw.loc[df_raw['odk_type']==s,['xml-parent','value']] # dataframe with help-fields / hint-fields only\n",
    "    drop_index = df_raw.loc[df_raw['odk_type']==s, 'id']\n",
    "    dfa.set_index('xml-parent', inplace = True) # in order to join dfa and df on index\n",
    "    sa = s[:-8]+'::en'\n",
    "    dfa.rename(columns = {'value':sa}, inplace = True) \n",
    "    df=df.join(dfa) # this adds the help message column to df\n",
    "    df.drop(drop_index, inplace = True) # remove 'help' rows from df (that data is now in the 'help' column)\n",
    "    \n",
    "df.fillna('', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca7383f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe that will be needed later to replace sources in df_arrows which are inside a container, by the container itself\n",
    "\n",
    "df_new_arrow_sources = df.loc[df['xml-parent'].isin(df.loc[df.odk_type=='container_hint_media'].index) \n",
    "                              | df['xml-parent'].isin(df.loc[df.odk_type=='container_page'].index),['xml-parent','odk_type']]\n",
    "df_new_arrow_sources.rename({'xml-parent':'container_id','odk_type':'odk_type_of_content'},axis=1,inplace=True)\n",
    "\n",
    "# add also the type of the container (page or hint-image)\n",
    "df_new_arrow_sources = df_new_arrow_sources.merge(df[['odk_type']],how='left',left_on='container_id',right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0578eb3c",
   "metadata": {},
   "source": [
    "### replace 'container_hint_media' labels with those of their children & drop children from df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15021a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_ids = df_raw[df_raw['odk_type']=='container_hint_media']['id']\n",
    "m = df_raw['xml-parent'].isin(container_ids) & ~df_raw['style'].str.contains('image',na=False) & ~df_raw['odk_type'].isin(['hint-message', 'help-message'])\n",
    "label_ids = list(df_raw[m]['id']) # used for dropping the labels from df after uploading info to container rows\n",
    "df_label = df_raw.loc[m, ['xml-parent','value','odk_type','name', 'id']] # all the label-children of containers \n",
    "df_label.set_index('xml-parent', inplace=True)\n",
    "# ATTENTION! df_raw still has duplicate names -> duplicates in df_label['name'], so fix it now:\n",
    "df_label.loc[df_label.duplicated(subset = ['name']), 'name'] = df_label['name'] + df_label['id']\n",
    "df.update(df_label) # update the containers' 'value', 'odk_type' and 'name'\n",
    "\n",
    "df.drop(label_ids, inplace = True) # drop the children from df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "783feb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for connectors where the source is inside a container-hint-media, replace the source with the container itself\n",
    "df_hint_media_objects = df_new_arrow_sources.loc[df_new_arrow_sources['odk_type']=='container_hint_media']\n",
    "df_arrows = df_arrows.merge(df_hint_media_objects,how='left',left_on='source',right_index=True)\n",
    "df_arrows.rename(columns={'odk_type':'container_type'},inplace=True)\n",
    "m=(df_arrows['container_type']=='container_hint_media')\n",
    "df_arrows.loc[m,'source']=df_arrows.loc[m,'container_id'] # replace the source by the container-hint-media itself\n",
    "df_arrows.loc[m,'source_type']=df_arrows.loc[m,'odk_type_of_content']\n",
    "df_arrows.drop(columns=['container_id','odk_type_of_content','container_type'],inplace=True)\n",
    "df_arrows.fillna('',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1197d7e",
   "metadata": {},
   "source": [
    "### new df_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7e079d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first you have to make yet another df_raw, just because in the original df_raw, the names are not yet dealth with duplicate\n",
    "# thing. \n",
    "df_raw2 = df_raw.copy()\n",
    "df_raw2.loc[df_raw2.duplicated(subset=['name'],keep='first') & ~df_raw2['name'].str.contains('opt_',na=False) \\\n",
    "       & ~df_raw2['name'].str.contains('stored_',na=False) \\\n",
    "       & ~df_raw2['name'].str.contains('shortcut_',na=False), 'name']=df_raw2['name']+df_raw2['id']\n",
    "\n",
    "\n",
    "# make df_choices\n",
    "# takes into account the right order of options in the drawing, based on 'y' value. \n",
    "# the py files written in spyder maintain compatibility to this script, notable the output has the 'id' of  the option and\n",
    "# the 'odk_type' --> as you move away from jupyter, modify the python functions, so that these are no longer included\n",
    "\n",
    "df_choices = oh.make_choicesframe(df_raw2)\n",
    "df_choices.rename(columns = {'label': 'label::en', 'image': 'image::en'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c406045",
   "metadata": {},
   "source": [
    "## to maintain compatibility with image extractor (remove as you fix this and harmonise image extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "671f7410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently the images have prefix 'png' or 'jpeg'. This will be deprecated, and the new df_choices does not add it\n",
    "# I add it here manually to maintain compatibility\n",
    "df_choices.loc[df_choices['image::en'].str.contains('.png', na=False),'image::en']='png-image' + df_choices['image::en']\n",
    "df_choices.loc[df_choices['image::en'].str.contains('.jpeg', na=False),'image::en']='jpeg-image' + df_choices['image::en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbd17184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the rows with 'choices' in df as they are no longer needed \n",
    "df.drop(df_choices.iloc[:-2].index,inplace=True)\n",
    "# drop the remaining unspecified objects (pure xml formating related elements or drawing artefacts) \n",
    "df.drop(df.loc[df.value==''].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0eebdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing df_arrows for logic part:\n",
    "\n",
    "# rename index of df_arrows to reduce confusion\n",
    "df_arrows.index.rename('Arrow ID',inplace=True)\n",
    "\n",
    "# make a logical expression for each arrow\n",
    "\n",
    "# add names of the source from df (for the case when the source is NOT a select_xxx) (names are the odk id's)\n",
    "# the value is only needed for the rhombus\n",
    "\n",
    "'''\n",
    "First we merge with df and then again with df_choices. The reason for that: at this stage, \n",
    "the arrows originate from select_xxx options (opt1,opt2,...), but do not point to them. \n",
    "However, at a later stage, those arrows are modified so they originate from the select_xxx itself. If that step was done \n",
    "before, we would not need to have to merge twice here. When improving the form builder, consider changing this. \n",
    "'''\n",
    "# merging with df to get the odk_type\n",
    "df_arrows=df_arrows.merge(df[['name','odk_type']],how='left',left_on='source',right_index=True)\n",
    "# moving the type of the source into the column 'source_type'\n",
    "df_arrows.loc[df_arrows['source_type']=='','source_type']=df_arrows.loc[df_arrows['source_type']=='','odk_type']\n",
    "# droping the 'odk_type' column, it is no longer needed\n",
    "df_arrows.drop(columns=['odk_type'],inplace=True)\n",
    "df_arrows.fillna('',inplace=True)\n",
    "\n",
    "# merging with df_choices to get the odk_type for when the source is a select_xxx\n",
    "df_arrows=df_arrows.merge(df_choices[['list_name','name','odk_type']],how='left',left_on='source',right_index=True)\n",
    "# as before for df, moving the type of the source into the column 'source_type'\n",
    "df_arrows.loc[df_arrows['source_type']=='','source_type']=df_arrows.loc[df_arrows['source_type']=='','odk_type']\n",
    "df_arrows.fillna('',inplace=True)\n",
    "\n",
    "# merge names from df and df_choices into one column\n",
    "df_arrows['source_name']=df_arrows['name_x']+df_arrows['list_name']\n",
    "df_arrows.drop(['name_x','list_name','odk_type'],axis=1,inplace=True)\n",
    "df_arrows.rename(columns={'name_y':'select_option'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa1e76",
   "metadata": {},
   "source": [
    "### Expression for each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6edf25b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arrows['expression']=''\n",
    "\n",
    "# add connectors to virtual objects (loaded objects)\n",
    "\n",
    "# expression for yes no questions\n",
    "df_arrows.loc[df_arrows['source_type']=='select_one yesno','expression'] = '${'+df_arrows['source_name'] + '}=' + '\\'' + df_arrows.value + '\\''\n",
    "\n",
    "# expression for integers and decimals\n",
    "#df_arrows.loc[(df_arrows['source_type']=='integer') | (df_arrows['source_type']=='decimal'),'expression'] = '${'+df_arrows['source_name'] + '}!=' + '\\'\\''\n",
    "# for integers and decimals that are NOT required, the expression must be removed:\n",
    "#m1 = df_arrows['source_type'].isin(['integer', 'decimal'])\n",
    "#m2 = df_raw['odk_type'].isin(['integer', 'decimal'])\n",
    "#df_arrow_int = df_arrows.loc[m1].reset_index().merge(df_raw.loc[m2, ['name', 'required']], how = 'left', left_on = 'source_name', right_on = 'name').set_index('Arrow ID')\n",
    "# merge with df_raw to get the 'required'\n",
    "#rowIDs = df_arrow_int.loc[df_arrow_int['required']=='no'].index\n",
    "#df_arrows.loc[rowIDs, 'expression']=''\n",
    "\n",
    "# expression for text-entry fields (the commented solution does not continue if the field is left empty)\n",
    "# df_arrows.loc[df_arrows['source_type']=='text','expression'] = '${'+df_arrows['source_name'] + '}!=' + '\\'\\''\n",
    "df_arrows.loc[df_arrows['source_type']=='text','expression'] = '(${'+df_arrows['source_name'] + '}!=' + '\\'\\' or ${'+df_arrows['source_name'] + '}=' + '\\'\\')'\n",
    "\n",
    "# expression for all the other select_one\n",
    "df_arrows.loc[df_arrows['source_type']=='select_one','expression'] = '${'+df_arrows['source_name'] + '}=' + '\\'' + df_arrows['select_option'] + '\\''\n",
    "\n",
    "# expression for select_multiple\n",
    "df_arrows.loc[df_arrows['source_type']=='select_multiple','expression'] = 'selected(${'+df_arrows['source_name'] + '},\\'' + df_arrows['select_option'] + '\\')'\n",
    "\n",
    "# expression for source being a calculate\n",
    "df_arrows.loc[df_arrows['source_type']=='calculate','expression'] = '${'+df_arrows['source_name'] + '}=1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebce5855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expression for target being a count---> in this case the expression depends not on the source but on the target!\n",
    "counters=df.loc[df['odk_type']=='count'].index\n",
    "m = df_arrows['target'].isin(df.loc[df['odk_type']=='count'].index) # mask for connectors that point to 'count' objects\n",
    "df_arrows.loc[m,'expression'] = 'number(' + df_arrows.loc[m,'expression'] + ')'\n",
    "\n",
    "# add arrow weight to counter\n",
    "m = df_arrows['value'].isin(['1','2','3']) & (df_arrows['target'].isin(df.loc[df['odk_type']=='count'].index))\n",
    "df_arrows.loc[m,'expression'] =  df_arrows.loc[m,'value'] + ' * ' + df_arrows.loc[m,'expression']\n",
    "\n",
    "# for counters you must combine the expression of all icoming arrows into the one expression of that counter. \n",
    "# from there on, a rhombus, referring to a counter can lookup the entire expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c25b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expression for rhombus\n",
    "\n",
    "m = df_arrows['source_type']=='rhombus'\n",
    "# remove prefix 'stored_'\n",
    "# ATTENTION! There is a BUG in pandas, replace(.... inplace = True) is not working!\n",
    "df_arrows.loc[m, 'source_name'] = df_arrows.loc[m, 'source_name'].replace(r'^stored_', r'', regex = True)\n",
    "\n",
    "# look up the odk_type that the rhombus is refering to\n",
    "df_arrows = df_arrows.merge(df[['odk_type','name']],how='left',left_on='source_name',right_on='name')\n",
    "# get rid of the 'name' column (was just needed for merging) and rename 'odk_type' column, to avoid confusion\n",
    "df_arrows.drop('name',axis=1,inplace=True)\n",
    "df_arrows.rename(columns={'odk_type':'rhombus_refer_to_odk_type'},inplace=True)\n",
    "\n",
    "# look up the value of the rhombus, it contains info about the logic\n",
    "df_arrows = df_arrows.merge(df[['value']],how='left',left_on='source',right_index=True)\n",
    "df_arrows.rename(columns={'value_x':'value','value_y':'value_of_rhombus'},inplace=True)\n",
    "# set all 'NaN' to empty strings\n",
    "df_arrows=df_arrows.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc2c0d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when rhombus refers to a an integer or decimal (OH BOY, IN YI, AGE IS A CALCULATE!!!)\n",
    "m = ((df_arrows['source_type']=='rhombus') & (df_arrows['rhombus_refer_to_odk_type'].isin(['integer','decimal']))) | (df_arrows['source_name']=='p_age')\n",
    "df_arrows.loc[m,'value_of_rhombus'] = df_arrows.loc[m,'value_of_rhombus'].str.replace(r'^[^<=>]+','',regex=True) # only keep what comes after <,= or >\n",
    "df_arrows.loc[m,'value_of_rhombus'] = df_arrows.loc[m,'value_of_rhombus'].str.replace('?','',regex=False) # remove the '?' at the end\n",
    "df_arrows.loc[m,'expression'] = '${'+df_arrows['source_name'] + '}' + df_arrows['value_of_rhombus']\n",
    "df_arrows.loc[m & (df_arrows['value']=='No')] = df_arrows.loc[m & (df_arrows['value']=='No')].replace({'<=':'>','>=':'<','<':'>=','>':'<='},regex=True)\n",
    "\n",
    "# when rhombus refers to a select_one yesno\n",
    "m = (df_arrows['source_type']=='rhombus') & (df_arrows['rhombus_refer_to_odk_type']=='select_one yesno')\n",
    "df_arrows.loc[m,'expression'] = '${'+df_arrows['source_name'] + '}=' + '\\'' + df_arrows.value + '\\''\n",
    "\n",
    "# now the real select_ones:\n",
    "# first line is for MSFeCARE PED, uncomment and comment 2nd when doing ped \n",
    "#m = (df_arrows['source_type']=='rhombus') & df_arrows['rhombus_refer_to_odk_type'].str.contains('select_',na=False) & (df_arrows['rhombus_refer_to_odk_type']!='select_one yesno') & (df_arrows['source_name']!='p_age')\n",
    "m = (df_arrows['source_type']=='rhombus') & df_arrows['rhombus_refer_to_odk_type'].str.contains('select_',na=False) & (df_arrows['rhombus_refer_to_odk_type']!='select_one yesno')\n",
    "df_arrows.loc[m,'value_of_rhombus'] = df_arrows.loc[m,'value_of_rhombus'].str.extract(r'\\[(.*?)\\]',expand=False)\n",
    "# merge again with df_choices to get the 'name' of the selected option (also needed for select_multiple!)\n",
    "df_arrows = df_arrows.merge(df_choices[['list_name','name','label::en']], \\\n",
    "                how='left',left_on=['source_name','value_of_rhombus'],right_on=['list_name','label::en'])\n",
    "# when the outgoing arrow is YES (means that what is in RHOMBUS is TRUE)\n",
    "df_arrows.loc[m & (df_arrows['value']=='Yes'),'expression'] =  '${'+df_arrows['source_name'] + '}=' + '\\'' + df_arrows['name'] + '\\''\n",
    "# when the outgoing arrow is NO (means that what is in RHOMBUS is FALSE)\n",
    "df_arrows.loc[m & (df_arrows['value']=='No'),'expression'] =  '${'+df_arrows['source_name'] + '}!=' + '\\'' + df_arrows['name'] + '\\''\n",
    "\n",
    "# when rhombus refers to select_multiple\n",
    "#m = (df_arrows['source_type']=='rhombus') & (df_arrows['rhombus_refer_to_odk_type']=='select_multiple')\n",
    "#df_arrows.loc[m,'value_of_rhombus'] = df_arrows.loc[m,'value_of_rhombus'].str.extract(r'\\[(.*?)\\]',expand=False)\n",
    "# when the outgoing arrow is YES (means that what is in RHOMBUS is TRUE)\n",
    "df_arrows.loc[m & (df_arrows['value']=='Yes'),'expression'] = 'selected(${'+df_arrows['source_name'] + '},\\'' + df_arrows['name'] + '\\')'\n",
    "# when the outgoing arrow is NO (means that what is in RHOMBUS is FALSE)\n",
    "df_arrows.loc[m & (df_arrows['value']=='No'),'expression'] = 'not(selected(${'+df_arrows['source_name'] + '},\\'' + df_arrows['name'] + '\\'))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b97ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when rhombus refers to calculate (AVOID THAT REFERENCE TO p_age GETS OVERWRITTEN!!! (SEE REFER TO INTEGER))\n",
    "m = (df_arrows['source_type']=='rhombus') & (df_arrows['rhombus_refer_to_odk_type']=='calculate') & (df_arrows['source_name']!='p_age')\n",
    "# when the outgoing arrow is YES (means that what is in RHOMBUS is TRUE)\n",
    "df_arrows.loc[m & (df_arrows['value']=='Yes'),'expression'] = '${'+df_arrows['source_name'] + '}=1'\n",
    "# when the outgoing arrow is NO (means that what is in RHOMBUS is False)\n",
    "df_arrows.loc[m & (df_arrows['value']=='No'),'expression'] = '${'+df_arrows['source_name'] + '}=0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c325e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when rhombus refers to a count (in this case we must combine all 'expressions' of the incoming arrows into the count object \n",
    "# with ' + ') and put the result into the 'expression' of the rhombus that is refering to it\n",
    "m = (df_arrows['source_type']=='rhombus') & (df_arrows['rhombus_refer_to_odk_type']=='count')\n",
    "df_arrows.loc[m,'value_of_rhombus'] = df_arrows.loc[m,'value_of_rhombus'].str.replace(r'^[^<=>]+','',regex=True) # only keep what comes after <,= or >\n",
    "df_arrows.loc[m,'value_of_rhombus'] = df_arrows.loc[m,'value_of_rhombus'].str.replace('?','',regex=False) # remove the '?' at the end\n",
    "\n",
    "# new mask to get the df_arrows of all connectors that point to counters\n",
    "m1 = df_arrows['target'].isin(df.loc[df['odk_type']=='count'].index) # mask for connectors that point to 'count' objects\n",
    "gk = df_arrows.loc[m1].groupby('target') # group them by counters\n",
    "\n",
    "for elem, group in gk:\n",
    "    # for each counter (elem), combine the expressions of all incoming arrows into a single one, concatenated with +\n",
    "    full_expression=' + '.join(filter(None,group['expression']))\n",
    "    # put result into brackets, because comparison is executed BEFORE +\n",
    "    full_expression = '(' + full_expression + ')'\n",
    "    \n",
    "    # lookup the 'name' of the counter in df, based on the id = target\n",
    "    counter_name = df.loc[elem,'name']\n",
    "    \n",
    "    # check in df_arrows where the source_name is 'counter_name'\n",
    "    # for the 'No' arrow we invert >, < and = of 'value of rhombus'\n",
    "    m2 = (df_arrows['source_name']==counter_name) & (df_arrows['value']=='No')\n",
    "    df_arrows.loc[m & m2,'value_of_rhombus'] = df_arrows.loc[m & m2,'value_of_rhombus'].replace({'<=':'>','>=':'<','<':'>=','>':'<=','=':'!=','!=':'='},regex=True)\n",
    "    df_arrows.loc[m & (df_arrows['source_name']==counter_name),'expression'] = full_expression + df_arrows['value_of_rhombus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4758cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also drop the arrows that point to counters\n",
    "df_arrows = df_arrows[df_arrows['target'].isin(df.loc[df['odk_type']!='count'].index)]\n",
    "\n",
    "# drop no longer necessary columns\n",
    "df_arrows.drop(columns=['value','value_of_rhombus','source_name','rhombus_refer_to_odk_type','list_name','label::en','name'],inplace=True)\n",
    "\n",
    "# also drop count objects from df, they are no longer needed\n",
    "df.drop(df[df['odk_type']=='count'].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8de22b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A rhombus can refer to a field that is not in the drawing. For instance, in the TT flow, where values like fever are used\\nbut not calculated. Or in CHT, when patient info or hospital info is loaded into the input section. \\nFor this, the symbols are drawn in the beginning of the flow, pointing to the note field 'Load Data'. \\nOnce this is done, it is handled correctly by the script and they get included. \""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''A rhombus can refer to a field that is not in the drawing. For instance, in the TT flow, where values like fever are used\n",
    "but not calculated. Or in CHT, when patient info or hospital info is loaded into the input section. \n",
    "For this, the symbols are drawn in the beginning of the flow, pointing to the note field 'Load Data'. \n",
    "Once this is done, it is handled correctly by the script and they get included. '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e768100",
   "metadata": {},
   "source": [
    "### Change sources that are 'select_options' to the 'select_xxx' itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d459b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the select_xxx for each select_option:\n",
    "dfa = df_raw.loc[df_raw['odk_type']=='select_option',['id', 'xml-parent']]\n",
    "# some select_xxx are in a container-hint-media, their ids have been replaced with the ids of the containers\n",
    "# therefore lookup the xml-parent of the select_xxx:\n",
    "dfa = dfa.merge(df_raw[['id', 'xml-parent']], how = 'left', left_on='xml-parent', right_on = 'id', suffixes=('', '_y'))\n",
    "# and if it is a container-hint-media, replace the 'xml-parent' of the select_option with the id of the container\n",
    "container_ids = list(df_raw.loc[df_raw['odk_type']=='container_hint_media', 'id'])\n",
    "m = dfa['xml-parent_y'].isin(container_ids)\n",
    "dfa.loc[m, 'xml-parent'] = dfa.loc[m, 'xml-parent_y']\n",
    "\n",
    "# make a dictionnary for replacing sources in df_arrows\n",
    "d = dict(zip(dfa.iloc[:,0], dfa.iloc[:,1]))\n",
    "df_arrows['source'].replace(d, inplace = True) # replace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a060536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for connectors where the source is inside a container-hint-media, replace the source with the container itself\n",
    "df_arrows = df_arrows.merge(df_new_arrow_sources,how='left',left_on='source',right_index=True)\n",
    "df_arrows.fillna('',inplace=True)\n",
    "df_arrows.rename(columns={'odk_type':'container_type'},inplace=True)\n",
    "m=(df_arrows['container_type']=='container_hint_media')\n",
    "df_arrows.loc[m,'source']=df_arrows.loc[m,'container_id']\n",
    "df_arrows.loc[m,'source_type']=df_arrows.loc[m,'odk_type_of_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f0e1db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get container_ids of pages\n",
    "container_ids = df_arrows.loc[df_arrows['container_type']=='container_page','container_id'].unique()\n",
    "\n",
    "# the ids of objects which are inside the page - containers\n",
    "page_objects = df.loc[df['xml-parent'].isin(container_ids)].index\n",
    "\n",
    "# get those page_objects which are the starting point of the flow INSIDE the page\n",
    "page_starts = page_objects[~page_objects.isin(df_arrows['target'])]\n",
    "\n",
    "# get the page_starts that are a rhombus (needed for later)\n",
    "page_starts_rhombus = df.loc[page_starts].loc[df['odk_type']=='rhombus'].index\n",
    "\n",
    "# get the page_objects where all objects in a single page are notes (needed for later)\n",
    "\n",
    "# get page_start - container_id pairs\n",
    "dfnew_connectors = df.loc[page_starts,['xml-parent']].reset_index().rename(columns={'id':'target','xml-parent':'source'})\n",
    "\n",
    "# add missing columns\n",
    "dfnew_connectors = dfnew_connectors.reindex(columns=['source','target','source_type','expression','container_id','container_type'])\n",
    "dfnew_connectors['source_type']='page'\n",
    "dfnew_connectors.fillna('',inplace=True)\n",
    "\n",
    "# concat that to df_arrows\n",
    "df_arrows = pd.concat([df_arrows,dfnew_connectors])\n",
    "\n",
    "# adding 'target_type' to df_arrows\n",
    "df_arrows = df_arrows.merge(df['odk_type'],how='left',left_on='target',right_index=True)\n",
    "df_arrows.rename(columns={'odk_type':'target_type'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac87f57",
   "metadata": {},
   "source": [
    "### Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33515d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# all connectors are present, we build the graph with networkx\\ndg = nx.from_pandas_edgelist(df_arrows, source='source', target='target', create_using=nx.DiGraph)\\n\\n# for a reason that I don't understand yet, if removing those two lines, I end up with empty brackets in some\\n# calcuations in the xlsx file, so leave it for now:\\nnodes_ordered = list(nx.lexicographical_topological_sort(dg))\\ndf=df.reindex(nodes_ordered)\\n\\n# check if there are loops\\nlist(nx.simple_cycles(dg))\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# all connectors are present, we build the graph with networkx\n",
    "dg = nx.from_pandas_edgelist(df_arrows, source='source', target='target', create_using=nx.DiGraph)\n",
    "\n",
    "# for a reason that I don't understand yet, if removing those two lines, I end up with empty brackets in some\n",
    "# calcuations in the xlsx file, so leave it for now:\n",
    "nodes_ordered = list(nx.lexicographical_topological_sort(dg))\n",
    "df=df.reindex(nodes_ordered)\n",
    "\n",
    "# check if there are loops\n",
    "list(nx.simple_cycles(dg))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d259c2c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e80cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a graph\n",
    "df_edges = df_raw.loc[df_raw['style'].str.contains('jettySize')]  # get all arrows from df_raw\n",
    "df_edges = df_edges[(df_edges['source']!='') & (df_edges['target']!='')] # remove some artefact objects\n",
    "\n",
    "dag = nx.from_pandas_edgelist(df_edges, source='source', target='target', create_using=nx.DiGraph) # build a graph\n",
    "\n",
    "# check if there are loops\n",
    "list(nx.simple_cycles(dag))\n",
    "\n",
    "# get id of Data Loader: \n",
    "dataloader_id = df_raw[df_raw['value']=='Load Data']['id'].iloc[0]\n",
    "# get edges that point to Data Loader:\n",
    "dataloaderedges = dag.in_edges(dataloader_id)\n",
    "# get parent nodes of data_loader\n",
    "dataloaderelements = [x[0] for x in list(dataloaderedges)]\n",
    "\n",
    "# drop data_loader and its predecessor nodes from dag\n",
    "dag.remove_node(dataloader_id)\n",
    "dag.remove_nodes_from(dataloaderelements)\n",
    "\n",
    "# drop image nodes from dag\n",
    "dag.remove_nodes_from(df_raw[df_raw['style'].str.contains('image',na=False)]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c418ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# must run several times, because there might be elements without a single edge, \n",
    "# for instance a select_multiple at the beginning of a page\n",
    "nodecount0 = 0\n",
    "nodecount = len(dag)\n",
    "\n",
    "while nodecount0 < nodecount:\n",
    "    # get elements without incoming edges \n",
    "    # -> these are page roots or select_options, or elements inside a container_hint_media, or images, or hints, helps\n",
    "    rootelements = [n for n,d in dag.in_degree() if d==0]  # elements that are origins\n",
    "\n",
    "    # get the parents of the rootelements (pages or select_xxx or container_hint_media)\n",
    "    m = df_raw['odk_type'].str.contains('container_',na=False) | df_raw['odk_type'].isin(['select_one', 'select_multiple'])\n",
    "    parent_ids = list(df_raw[m]['id']) # ids of parents of root elements:\n",
    "    df_roots = df_raw.loc[df_raw['id'].isin(rootelements) & df_raw['xml-parent'].isin(parent_ids)]  \n",
    "    parent_root_edges = list(zip(df_roots['xml-parent'],df_roots['id']))\n",
    "\n",
    "    # add parent_to_root_edges to dag\n",
    "    dag.add_edges_from(parent_root_edges)\n",
    "    nodecount0 = nodecount\n",
    "    nodecount = len(dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "951f5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking into account shortcuts\n",
    "\n",
    "df_shortcuts = df_raw.loc[df_raw['odk_type']=='goto',['id','name']]\n",
    "df_shortcuts.replace({'name': r'^shortcut_'}, {'name': ''}, regex=True, inplace=True)\n",
    "exit_nodes = df_raw[df_raw['name'].isin(df_shortcuts['name'].unique())]\n",
    "exit_nodes_dict = dict(zip(exit_nodes['name'], exit_nodes['id']))\n",
    "\n",
    "# dictionnary representing the edge towards a shortcut\n",
    "node_to_shortcut = [x for x in dag.edges() if x[1] in list(df_shortcuts['id'])]\n",
    "node_to_shortcut = dict(zip([x[1] for x in node_to_shortcut], [x[0] for x in node_to_shortcut]))\n",
    "\n",
    "df_shortcuts['name'].replace(exit_nodes_dict, inplace = True) # replacing name of exit node by its id\n",
    "df_shortcuts['id'].replace(node_to_shortcut, inplace = True) # replacing id of shortcut by id of parent node\n",
    "# df_shortcuts now represents edges to be added to dag\n",
    "\n",
    "# drop shortcut nodes in dag\n",
    "shortcut_ids = df_raw[df_raw['odk_type']=='goto']['id']\n",
    "dag.remove_nodes_from(shortcut_ids)\n",
    "\n",
    "# add new edges\n",
    "shortcut_edges = list(zip(df_shortcuts['id'], df_shortcuts['name']))\n",
    "dag.add_edges_from(shortcut_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262f8f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24192b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% DAG \n",
    "# build a CDSS graph without images, WITH dataloader\n",
    "dag = gt.build_graph_cdss(df_raw)\n",
    "\n",
    "# make edge parents -> children (for select_xxx and pages and container-hint-media this does not exist per default)\n",
    "dag = gt.connect_to_parents_old(dag, df_raw)\n",
    "\n",
    "# connect shortcuts\n",
    "dag = gt.connect_shortcuts(dag, df_raw)\n",
    "\n",
    "# assign 'type', 'name', 'value' and group membership as attributes to nodes\n",
    "if form_id == 'almsom': # in Somalia TT, the 'name' is in the content\n",
    "    dag = gt.add_nodeattrib(dag, df_raw['id'], df_raw['value'], 'name')\n",
    "    dag = gt.add_nodeattrib(dag, df_raw['id'], df_raw['odk_type'], 'type')\n",
    "    dag = gt.add_nodeattrib(dag, df_raw['id'], df_raw['xml-parent'], 'group')\n",
    "    #dag = gt.add_nodeattrib(dag, df_raw['id'], df_raw['y'], 'y')\n",
    "else: \n",
    "    dag = gt.add_nodeattrib(dag, df_raw['id'], df_raw['name'].apply(ch.html2plain), 'name')\n",
    "    dag = gt.add_nodeattrib(dag, df_raw['id'], df_raw['odk_type'], 'type')\n",
    "    # if you want to strip off html from text:\n",
    "    # dag = gt.add_nodeattrib(dag, df_raw['id'], df_raw['value'].apply(ch.html2plain), 'content')\n",
    "    # if you want to keep the html in the text:\n",
    "    dag = gt.add_nodeattrib(dag, df_raw['id'], df_raw['value'], 'content')\n",
    "    dag = gt.add_nodeattrib(dag, df_raw['id'], df_raw['xml-parent'], 'group')\n",
    "    dag = gt.add_nodeattrib(dag, df_raw['id'], df_raw['y'], 'y')\n",
    "\n",
    "# assign content of edges as their 'logic' attribute -> there are edges in the form that contain 'Yes' or 'No'\n",
    "dag = gt.add_edgeattrib(dag, df_raw, 'logic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb390fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b0e077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500d60e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a701660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ecc846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84c7100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c5fac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b93e522",
   "metadata": {},
   "source": [
    "## CDSS topological sort of the global graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c68767b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention, more than 1 graph entry point found, this is not allowed\n"
     ]
    }
   ],
   "source": [
    "# to make legacy solution work with new functions, make columns with new names in df_raw\n",
    "df_raw['parent'] = df_raw['xml-parent']\n",
    "\n",
    "diagnosis_id_hierarchy = gt.get_diagnosis_sorting_id_jupyter(df_raw, diagnose_order) # make diagnosis id hierarchy list\n",
    "\n",
    "opt_prio = gt.hierarchy_select_options(df_raw) # hierarchy of select_options in the form\n",
    "# combine diagnosis_sorting with select_option sorting\n",
    "#opt_prio = d | opt_prio\n",
    "\n",
    "# add an edge between dataloader and the first node of the form\n",
    "dataloader_id = list(df_raw.loc[df_raw['value']=='Load Data', 'id'])[0]\n",
    "rootelements = [n for n,d in dag.in_degree() if d==0]  # elements that are origins\n",
    "if len([n for n in rootelements if dataloader_id not in list(dag.successors(n))]) != 1:\n",
    "    print('Attention, more than 1 graph entry point found, this is not allowed')\n",
    "\n",
    "non_dataloader_rootelement = [n for n in rootelements if dataloader_id not in list(dag.successors(n))][0]\n",
    "dag.add_edge(dataloader_id, non_dataloader_rootelement)\n",
    "\n",
    "rootelement = gt.get_graph_entry_point(dag)\n",
    "\n",
    "# get the global, flattened list of nodes sorted accordingly to universal graph\n",
    "sorting = gt.get_topo_sort_cdss(dag, rootelement, opt_prio)\n",
    "\n",
    "# keep load_data elements from df in dfa because they will get dropped during sorting\n",
    "#dfa = df[~df.index.isin(dag.nodes)]\n",
    "#df.drop(dfa.index, inplace=True)\n",
    "I = df.index \n",
    "# only keep nodes from nodes_sorted that also exist in df (drop those that only exist in df_raw)\n",
    "I_new = [node for node in sorting if node in I]\n",
    "\n",
    "df = df.reindex(I_new) # df is now sorted, but there are many rows from df_raw, that no longer exist in df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6fb8fc",
   "metadata": {},
   "source": [
    "### Build the 'relevant' logic for each object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba094e5c",
   "metadata": {},
   "source": [
    "Building the logic: \n",
    "1. It must be done for each object independently, not for all at once, so there is a for loop\n",
    "2. Start on the very top and go down the tree. This is the reason why we have topologically sorted df in the previous step\n",
    "3. For each object lookup all sources in df_arrows (get all rows from df_arrows where the object is the target). \n",
    "4. Each source -> target arrow has a logic expression and the entire 'relevant' of the target is just the logic expressions of \n",
    "    all incoming arrows, combined with a OR. \n",
    "5. A particular attention must be paid when a source is a 'note': then the 'expression' is empty. \n",
    "    That is because there is no decision taken for 'notes', there is only one arrow coming out from a note. \n",
    "    In this case we must use the relevant of the 'note' and 'calculate' source itself as the expression of note -> target\n",
    "    This would also be the case for 'calculate' objects, but their 'expression' has been populated already.  \n",
    "    If we do not do that, then the target would pop up independently of the 'note/calculate' condition. That would be wrong. \n",
    "    Therefore, in df_source, the 'expression' for 'note' and 'calculate' is the 'relevant' of those sources. \n",
    "    To get those into df_sources, we merge it with df accordingly. \n",
    "    Therefore it is also important to do the logic from top to bottom, to assure that the relevant of the previous objects \n",
    "    has already been done. \n",
    "6. Another particular interest is for rhombus (previously entered data). Here we also need the relevant of the rhombus \n",
    "    itself, because it must be combined with the expresion by an AND. The rhombus itself is not seen to the user, \n",
    "    so the logic depends on his relevant. For the terms to be executed in the right order, the 'relevant' must be put \n",
    "    into brackets first. \n",
    "7. After those steps we have a df_sources dataframe where the 'expression' is correct for each of the arrows (each row). \n",
    "    As said in (4) they are combined with OR and written into the 'relevant' of the object we are looking at. \n",
    "8. Another major problem are pages that contain ONLY notes. As objects inside a page automatically inherit the relevant \n",
    "    of the page itself, their expression is entirely empty, they are only governed by the relevant of the page. \n",
    "    At the exit of such a page, a note without an expression is pointing to a target outside the page. \n",
    "    The following object would then always be displayed (or never, if there are other arrows pointing to)\n",
    "    To deal with this we identify all those objects (groups that contain only notes and )\n",
    "9. Another problem is when the first object in a page is a rhombus. It also gets no relevant generated. As a consequence, \n",
    "    we would get just the expression with 'and ()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ae8ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is necessary because there are pages that contain 'note' fields only. \n",
    "# In this case notes that point ouf of the page, have no 'expression'. This interrupts the flow. \n",
    "# The solution is to give those 'notes' as expression the 'relevant' of the page\n",
    "\n",
    "df_pageObjects = df.loc[df['xml-parent'].isin(df.loc[df['odk_type']=='container_page'].index)]\n",
    "\n",
    "# get ids of pages that ONLY contain 'notes'\n",
    "pure_note_pages=[]\n",
    "gk = df_pageObjects.groupby('xml-parent')\n",
    "for elem,frame in gk: \n",
    "    if len(frame.index) == len(frame.loc[frame['odk_type']=='note']):\n",
    "        pure_note_pages.append(elem)\n",
    "\n",
    "# get all the 'notes' that point out pages:\n",
    "df_notes_out_pages = df_arrows.loc[df_arrows['source'].isin(df_pageObjects.index) & \\\n",
    "                                    ~df_arrows['target'].isin(df_pageObjects.index) & (df_arrows['source_type']=='note')]\n",
    "\n",
    "# among those get those notes that belong to 'pure_note_pages' - these are the notes you are looking for\n",
    "df_notes_outof_pure_notes_pages = df_notes_out_pages.loc[df_notes_out_pages['container_id'].isin(pure_note_pages)]\n",
    "df_notes_outof_pure_notes_pages = df_notes_outof_pure_notes_pages[['source','container_id']]\n",
    "df_notes_outof_pure_notes_pages.set_index('source',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9a24e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['relevant']=''\n",
    "\n",
    "for elem in df.index:     \n",
    "    # df_sources: dataframe that contains all connections pointing to the object 'elem'\n",
    "    df_sources = df_arrows.loc[df_arrows['target']==elem,['source','source_type','expression']]\n",
    "    # pulling the relevant of the sources into df_sources. This corresponds to the logic to each elem. \n",
    "    # 'xml-parent' is needed for rhombus at beginning of a page\n",
    "    df_sources = df_sources.merge(df[['relevant','xml-parent']],how='left',left_on='source',right_index=True) \n",
    "\n",
    "    # when the source is a rhombus and it's relevant IS empty and the rhombus is on a page\n",
    "    # you have to combine the expression with the relevant of the page\n",
    "    # first merge with df again to the the relevant of the page\n",
    "    df_sources = df_sources.merge(df[['relevant']],how='left',left_on='xml-parent',right_index=True,suffixes=('', '_page'))\n",
    "    m=df_sources['source_type'].isin(['rhombus']) & (df_sources['relevant']=='') & df_sources['xml-parent'].isin(container_ids)\n",
    "    df_sources.loc[m,'expression'] = df_sources.loc[m,'expression'] + ' and (' + df_sources.loc[m,'relevant_page'] + ')'    \n",
    "    \n",
    "    # when the source is a rhombus and it's relevant is NOT empty, you have to combine both with AND\n",
    "    m=df_sources['source_type'].isin(['rhombus']) & (df_sources['relevant']!='')\n",
    "    df_sources.loc[m,'expression'] = df_sources.loc[m,'expression'] + ' and (' + df_sources.loc[m,'relevant'] + ')'\n",
    "    \n",
    "    # when the source is a note, just take its relevant and put it into expression\n",
    "    m=df_sources['source_type'].isin(['note'])\n",
    "    df_sources.loc[m,'expression'] = df_sources.loc[m,'relevant']    \n",
    "\n",
    "    # when the source is a note that is pointing out of a page that only contains 'notes' use the page relevant as \n",
    "    # its expression \n",
    "    m=df_sources['source'].isin(df_notes_outof_pure_notes_pages.index) # Mask to get pure note elements\n",
    "    df_sources.loc[m,'expression'] = df_sources.loc[m,'relevant_page']\n",
    "    #page_id = df.loc[df_sources.loc[m,'source'],'xml-parent'] # # get the page_ids of the pages the elments are in\n",
    "    # df_sources.loc[m,'expression'] = df.loc[page_id,'relevant'].to_list()\n",
    "    \n",
    "    df_sources.loc[df_sources['expression']!='','expression'] = '(' + df_sources.loc[df_sources['expression']!='','expression'] + ')'\n",
    "    \n",
    "    if df.loc[elem,'odk_type']!='count':\n",
    "        expressions = set(filter(None,df_sources['expression'])) # to reduce repetitions in expression\n",
    "        df.loc[elem,'relevant'] = ' or '.join(expressions)\n",
    "        if '( and (' in df.loc[elem,'relevant'] or '( or (' in df.loc[elem,'relevant']:\n",
    "            print(elem, 'error!')\n",
    "            print(df.loc[elem,'relevant'])\n",
    "    else:\n",
    "        # for counters the joining is number + number\n",
    "        df.loc[elem,'relevant'] = ' + '.join(filter(None,df_sources['expression'])) \n",
    "        if '( and (' in df.loc[elem,'relevant'] or '( or (' in df.loc[elem,'relevant']:\n",
    "            print(elem, 'error!')\n",
    "            print(df.loc[elem,'relevant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8646ab3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1a1722e",
   "metadata": {},
   "source": [
    "### Taking into account pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30d66901",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The topological sorting does not take into account pages (page-containers). Objects that are on the same page, must be \n",
    "grouped in order to wrap them up in begin_group ... end_group in odk. The topological_sort does not know what. \n",
    "Therefore we resort df: all objects that belong the a page, get all ligned up below the page container, \n",
    "preserving their overall sorting in df.\n",
    "'''\n",
    "pageids = df.loc[df['odk_type']=='container_page'].index\n",
    "df.reset_index(drop=False, inplace=True)\n",
    "df['new_index']=pd.to_numeric(df.index)\n",
    "pagerows = df.loc[df['odk_type']=='container_page'].index\n",
    "df = df.merge(df[['id','new_index']], how='left', left_on='xml-parent', right_on='id', suffixes = ('', '_p'))\n",
    "df.loc[df['new_index_p'].notna(), 'new_index'] = df['new_index_p']\n",
    "df.drop(columns=['id_p','new_index_p'], inplace=True)\n",
    "\n",
    "g = df.groupby('xml-parent') # group by pages\n",
    "for name, frame in g:  # for each page\n",
    "    k=0.001\n",
    "    for i in frame.index: # for each element in that page\n",
    "        if df.loc[i,'xml-parent'] in pageids: # if we are in a real page and not in root\n",
    "            df.loc[i,'new_index'] = df.loc[i,'new_index']+k # add to the new index a small step\n",
    "            k+=0.001\n",
    "            \n",
    "df.set_index('new_index', drop=True, inplace = True)\n",
    "df.sort_index(inplace=True)\n",
    "df.set_index('id', drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee15cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 'end group' rows\n",
    "# get the last objects of each page\n",
    "df.reset_index(drop=False, inplace=True)\n",
    "index_endgrouprows = df.loc[~df.duplicated(subset='xml-parent', keep='last') & df['xml-parent'].isin(pageids)].index+0.1\n",
    "\n",
    "df_endgroup = pd.DataFrame(index=index_endgrouprows)\n",
    "df_endgroup['odk_type']='end group'\n",
    "df_endgroup['id']=df_endgroup.index\n",
    "\n",
    "df = pd.concat([df_endgroup, df])\n",
    "df.sort_index(inplace=True)\n",
    "df.set_index('id', drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b3f3779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# short term workaround for select_xxx + NAME to add the same name as list_name\n",
    "m = df['odk_type'].isin(['select_one','select_multiple'])\n",
    "df.loc[m,'odk_type'] = df.loc[m,'odk_type'] + ' ' + df.loc[m,'name']\n",
    "\n",
    "# making df look like the 'survey' tab in an xls form\n",
    "df[['repeat_count','appearance','required message::en','calculation']]=''\n",
    "df=df[['odk_type','name','value','help::en','hint::en','appearance','relevant','constraint', \\\n",
    "       'constraint_message','required','required message::en','calculation','repeat_count','image::en']]\n",
    "df.rename(columns={'odk_type':'type','value':'label::en','relevant':'relevance','constraint_message':'constraint message::en'},inplace=True)\n",
    "\n",
    "# rename begin group\n",
    "df.replace({'container_page':'begin group'}, inplace=True)\n",
    "# add 'field-list'\n",
    "df.loc[df['type']=='begin group','appearance']='field-list'\n",
    "\n",
    "# keep the ids of the rhombus and drop the rhombus objects from df\n",
    "# drop rhombus\n",
    "df.drop(df.loc[df['type']=='rhombus'].index,inplace=True)\n",
    "\n",
    "# in 'calculate' fields move 'relevance' to calculate\n",
    "df.loc[df['type']=='calculate','calculation'] = df.loc[df['type']=='calculate','relevance']\n",
    "# add 'number() to fit with odk '\n",
    "df.loc[df['type']=='calculate','calculation'] = 'number(' + df.loc[df['type']=='calculate','calculation'] + ')'\n",
    "# delete entry in relevance column of 'calcuate' rows\n",
    "df.loc[df['type']=='calculate','relevance'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9293aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making df_choices look like the 'choices' tab in an xls form\n",
    "df_choices.drop(columns=['odk_type'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec8db828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>form_title</th>\n",
       "      <th>form_id</th>\n",
       "      <th>version</th>\n",
       "      <th>default_language</th>\n",
       "      <th>style</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pediatric</td>\n",
       "      <td>ped</td>\n",
       "      <td>202302091423</td>\n",
       "      <td>en</td>\n",
       "      <td>pages</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  form_title form_id       version default_language  style\n",
       "1  Pediatric     ped  202302091423               en  pages"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a 'settings' tab\n",
    "now = datetime.now()\n",
    "version=now.strftime('%Y%m%d%H%M')\n",
    "indx=[[1]]\n",
    "\n",
    "settings={'form_title':form_title,'form_id':form_id,'version':version,'default_language':'en','style':'pages'}\n",
    "df_settings=pd.DataFrame(settings,index=indx)\n",
    "df_settings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a264d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the diagnoses and the corresponding ids\n",
    "df_diagnoses = pd.read_csv(diagnose_order)\n",
    "diagnoses_dict=dict(zip(df_diagnoses.Name,df_diagnoses.id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b322f4b",
   "metadata": {},
   "source": [
    "## make standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b6a9c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding top questions and populating the 'calculate' column of the calculate fields in order to make the treatment flow \n",
    "# STANDALONE\n",
    "\n",
    "# making the top questions \n",
    "\n",
    "# for the diagnostic flow as a shortterm we extract all the 'calculates' where the tooltip starts with 'load_'\n",
    "# this is because, at this stage we no longer can distinguish the data-load-calculates from the normal calcualtes\n",
    "# drawback is that now all data-loaders must have a tooltip starting with 'load_'. In the future this will be fixed, probably \n",
    "# by adding a new data_attribute 'load_data' and make a special data_loader object\n",
    "tt_input_options = df.loc[(df['type']=='calculate') & df['name'].str.contains('load_',na=False),['type','name','label::en']]\n",
    "\n",
    "\n",
    "tt_input_options.rename(columns={'type':'list_name'},inplace=True) # tt_input_options are 'contextual parameters'\n",
    "tt_input_options['list_name']='data_load'\n",
    "df_choices = pd.concat([df_choices,tt_input_options]) # concat the new options to df_choices\n",
    "\n",
    "# make the first question for data load\n",
    "# data_load = ['select_multiple calculate','data_load','Define adaptable parameters','','','','','','','','','','','']\n",
    "data_load = ['select_multiple data_load','data_load','Define adaptable parameters','','','','','','','','','','','']\n",
    "\n",
    "data_load = pd.DataFrame([data_load],columns=df.columns)\n",
    "df = pd.concat([data_load,df])\n",
    "\n",
    "# populate the load_ calculate fields\n",
    "df.loc[(df['type']=='calculate') & df['name'].str.contains('load_',na=False),'calculation']='number(selected(${data_load}, \\''+ df.loc[df['type']=='calculate','name'] + '\\'))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d0b273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd3ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3292af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a6f0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9eddbc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate constraint message to all select_multiple\n",
    "df.loc[df['type'].str.contains('select_multiple',na=False),'constraint']='.=\\'opt_none\\' or not(selected(.,\\'opt_none\\'))'\n",
    "df.loc[df['type'].str.contains('select_multiple',na=False),'constraint message::en']='**None** cannot be selected together with symptoms.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f951532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load zscore_file containing drug dosages and zscore calculations into df\n",
    "df = loadcalc(df, drugsfile, form_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "33bcb551",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "From CHT Docs\n",
    "\n",
    "Countdown Timer: A visual timer widget that starts when tapped/clicked, and has an audible alert when done. \n",
    "To use it create a note field with an appearance set to countdown-timer. \n",
    "The duration of the timer is the fields value, which can be set in the XLSForms default column. \n",
    "If this value is not set, the timer will be set to 60 seconds.\n",
    "\n",
    "Currently not implemented in TRICC, but hard coded here\n",
    "'''\n",
    "df.loc[df['name']=='label_rr_rate','appearance']='countdown-timer'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747adf6e",
   "metadata": {},
   "source": [
    "## quick fix hardcode some stuff for YI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "adccb489",
   "metadata": {},
   "outputs": [],
   "source": [
    "if form_id == 'yi':\n",
    "    # hardcode calculation of age 'p_age'\n",
    "    df.loc[df['name']=='p_age','calculation']= 'coalesce(${p_age_days},${p_age_weeks}*7)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c905972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the Data Loader field, if not it will appear in the flow. \n",
    "df.drop(df.loc[df['label::en']=='Load Data'].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9c2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec77f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b8d3d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a89fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe42c36b",
   "metadata": {},
   "source": [
    "### add a 'diagnosis found' message immediately after the diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8571a2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the detected diagnosis right on detection\n",
    "df.reset_index(inplace=True)\n",
    "df.fillna('',inplace=True)\n",
    "I = df.loc[df['name'].isin(diagnoses_dict.values())].index\n",
    "\n",
    "for i in I:\n",
    "    d_message = pd.DataFrame({'index':df.loc[i]['index']+'_dm','type': 'note', \\\n",
    "                                'name':'dm_' + df.loc[i]['name'],'label::en':\\\n",
    "                                'Diagnosis found: ' + df.loc[i]['label::en'],\\\n",
    "                                'relevance':'number(${'+df.loc[i]['name']+'})=1'}, index=[i+0.1])\n",
    "    \n",
    "    #df = df.append(d_message, ignore_index=False)\n",
    "    df = pd.concat([df, d_message], ignore_index = False)\n",
    "\n",
    "\n",
    "# colorize the dm message\n",
    "m = df['name'].str.contains('dm_',na=False)\n",
    "df.loc[m,'label::en'] = '<span style=\"color: rgb(163, 92, 56);\">' + df.loc[m,'label::en'] + '</span>'\n",
    "\n",
    "# sort rows and reset index\n",
    "df = df.sort_index()\n",
    "df.set_index('index',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f9f34",
   "metadata": {},
   "source": [
    "### Change appearance of help fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce3928f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% For CHT put help fields in a standard note field, just below the row the help is attached to\n",
    "df = oh.helpfields(df)\n",
    "\n",
    "# LEGACY\n",
    "#from helpfields import helpfields\n",
    "#df.fillna('', inplace = True)\n",
    "#df = helpfields(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe0681",
   "metadata": {},
   "source": [
    "### add required = true to all data entry fields shortterm to save some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ca284a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[~df['type'].isin(['note','calculate','begin group','end group','text', 'acknowledge', '']) & (df['required']==''),'required']='true()'\n",
    "\n",
    "# but not to some and contextual parameters ()\n",
    "skip = ['data_load', 'p_height', 'p_length', 'p_o2']\n",
    "\n",
    "df.loc[df['name'].isin(skip),'required']='false()'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f34ce9",
   "metadata": {},
   "source": [
    "### for ped, add the proper p_age calculate row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b2f93285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 'ped' hardcode a 'p_age' calculate\n",
    "if form_id == 'ped':\n",
    "    df.loc[df['name']=='p_age', 'calculation'] = \"if(${p_age_select}='2',2,if(${p_age_select}='4',4,if(${p_age_select}='6',6,if(${p_age_select}='12',12,if(${p_age_select}='24',24,if(${p_age_select}='36',36,48))))))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f38d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e6f220d",
   "metadata": {},
   "source": [
    "### combine multiple instances of a calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "782f1f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Ceftriaxone the new calculation expression is:\n",
      " number(number((${d_scm}=1)) or number((${d_uti_febrile}=1)) or number((${d_anaphylaxis_severe}=1)) or number((${d_disease_severe_resipartory}=1)) or number((${d_mastoiditis}=1)) or number((${d_anemia_severe}=1)) or number((${d_malaria_severe}=1)) or number((${d_pneumonia_severe}=1)) or number((${d_varicella_severe_chickenpox}=1)) or number((${d_uvulectomy_complicated}=1)) or number((${d_malaria_severe_possible}=1)) or number((${d_cellulitis_periorbital_orbital}=1) or (${d_abscess_multiple_large}=1) or (${d_cellulitis_febrile}=1)) or number((${d_dehydration_severe}=1)) or number((${d_dysentery_complicated}=1)) or number((${d_pain_abdominal_severe}=1)) or number((${d_vsd}=1)))\n",
      "The name of the combined calculate field is: d_ceftriaxone\n",
      "For Referral (red diagnosis) the new calculation expression is:\n",
      " number(number((${d_abscess_multiple_large}=1) or (${d_cellulitis_febrile}=1)) or number((${d_dysentery_complicated}=1) or (${d_intussusception_possible}=1)) or number((${d_uvulectomy_complicated}=1)) or number((${d_lesions_corneal_possible}=1)) or number((${d_stomatitis}=1)) or number((${d_dehydration_complicated}=1)) or number((${d_anaphylaxis_severe}=1)) or number((${d_disease_severe_resipartory}=1)) or number((${d_anaphylaxis}=1) or (${d_vsd_dehydrated}=1)) or number((${d_nutritional_risk_high}=1) or (${d_anemia_severe}=1)) or number((${d_vsd}=1)) or number((${d_mastoiditis}=1)) or number((${d_pain_abdominal_severe}=1) or (${d_hernia_inguinal_strangulated_possible}=1) or (${d_torsion_testicular_risk}=1)) or number((${d_dehydration_severe}=1)) or number((${d_scm}=1)) or number((${d_cellulitis_periorbital_orbital}=1)) or number((${d_anaphylaxis}=1) or (${d_wheezing_resistant}=1)) or number((${d_malnutrition_acute_complications}=1)) or number((${d_pneumonia_severe}=1)) or number((${d_varicella_severe_chickenpox}=1)) or number((${d_vsd_stridor}=1) or (${d_vsd_malnutrition_acute_complications}=1) or (${d_vsd_difficult_breathing}=1)))\n",
      "The name of the combined calculate field is: d_referral_red\n",
      "For Anaphylaxis the new calculation expression is:\n",
      " number(number((${ask_wheezing_hives}='Yes')) or number((selected(${select_prereferal_details},'opt_2') and ((not(selected(${select_prereferal_details},'opt_1')) and ((selected(${select_prereferal_details},'opt_4')) or (selected(${select_prereferal_details},'opt_3')))))) or (selected(${select_prereferal_details},'opt_1') and ((selected(${select_prereferal_details},'opt_4')) or (selected(${select_prereferal_details},'opt_3'))))))\n",
      "The name of the combined calculate field is: d_anaphylaxis\n",
      "For MalariaRDT needed the new calculation expression is:\n",
      " number(number((${load_malaria_transmission_area}=1 and ((${d_pneumonia_severe}=1) or (${d_wheezing_resistant}=1)))) or number((${load_malaria_transmission_area}=1 and ((${d_vsd}=1 and ((${d_anaphylaxis_evolution_positive}=1)))))) or number((${load_malaria_transmission_area}=1 and ((${d_anemia_none_severe}=1)))) or number((${load_malaria_transmission_area}=1 and ((${d_anemia_severe}=1)))) or number((${load_malaria_transmission_area}=1 and ((selected(${major_symptoms},'opt_1'))))) or number((${load_malaria_transmission_area}=1 and ((${d_vsd_malnutrition_acute_complications}=1) or (${d_vsd_difficult_breathing}=1) or (${d_vsd_stridor}=1) or (not(selected(${select_prereferal_details},'opt_2')) and ((not(selected(${select_prereferal_details},'opt_1')) and ((selected(${select_prereferal_details},'opt_4')) or (selected(${select_prereferal_details},'opt_3')))))) or (selected(${select_prereferal_details},'opt_none')) or (${d_vsd_dehydrated}=1)))) or number((${load_malaria_transmission_area}=1 and ((${d_fever}=1)))) or number((${load_malaria_transmission_area}=1 and ((${d_fever_past_present}=1 and ((not(selected(${major_symptoms},'opt_4')) and ((${ask_abdominal_pain_pcm}='No') or (${ask_refuse_abdominal_palpatation}='No')))))))))\n",
      "The name of the combined calculate field is: d_malariaRDT\n",
      "For Urinalysis needed the new calculation expression is:\n",
      " number(number((${d_runny_nose_ear_inf}=0 and (((number(selected(${major_symptoms},'opt_1')) + number(selected(${major_symptoms},'opt_2')) + number(selected(${major_symptoms},'opt_3')) + number(selected(${major_symptoms},'opt_4')) + number(selected(${major_symptoms},'opt_5')) + number(selected(${major_symptoms},'opt_6')))=1 and ((${p_age}<24 and ((${load_malaria_transmission_area}=1 and ((selected(${major_symptoms},'opt_1')))) or (${load_malaria_transmission_area}=0 and ((selected(${major_symptoms},'opt_1'))))))))))) or number((${load_malaria_transmission_area}=1 and ((${d_fever_past_present}=1 and ((not(selected(${major_symptoms},'opt_4')) and ((${ask_abdominal_pain_pcm}='No') or (${ask_refuse_abdominal_palpatation}='No'))))))) or (${d_fever_past_present}=0 and ((not(selected(${major_symptoms},'opt_4')) and ((${ask_abdominal_pain_pcm}='No') or (${ask_refuse_abdominal_palpatation}='No'))))) or (not(selected(${major_symptoms},'opt_4')) and ((${ask_abdominal_pain_pcm}='No') or (${ask_refuse_abdominal_palpatation}='No'))) or (${load_malaria_transmission_area}=0 and ((${d_fever_past_present}=1 and ((not(selected(${major_symptoms},'opt_4')) and ((${ask_abdominal_pain_pcm}='No') or (${ask_refuse_abdominal_palpatation}='No')))))))) or number((selected(${select_symptoms_other},'opt_9'))) or number((${d_runny_nose_ear_inf}=0 and ((${d_referral_red}=0 and ((${p_age}<24 and ((${load_malaria_transmission_area}=0 and ((${d_fever}=1))) or (${load_malaria_transmission_area}=1 and ((${d_fever}=1)))))))))))\n",
      "The name of the combined calculate field is: d_urinalysis\n",
      "For Present Fever or Fever History the new calculation expression is:\n",
      " number(number((selected(${major_symptoms},'opt_1'))) or number((${p_temp}>37.5 and ((${d_deceased}=0)))))\n",
      "The name of the combined calculate field is: d_fever_past_present\n",
      "For Upper respiratory tract infection the new calculation expression is:\n",
      " number(number((${d_fever_past_present}=0 and ((${respiratory_rate}<40 and ((${p_age}>=12 and ((${respiratory_rate}>0 and ((${p_age}>=6 and ((selected(${select_symptom_respiratory},'opt_none')))) or (${ask_cyanosis}='No'))))))) or (${respiratory_rate}<50 and ((${p_age}<12 and ((${respiratory_rate}>0 and ((${p_age}>=6 and ((selected(${select_symptom_respiratory},'opt_none')))) or (${ask_cyanosis}='No'))))))))) or (${ask_indrawing_chestwall}='No')) or number((${d_measles_any}=0 and ((${d_pneumonia}=0 and ((${d_bronchitis}=0 and ((${d_runny_nose}=1)))))))))\n",
      "The name of the combined calculate field is: d_urti\n",
      "For Unclassified skin problem (likely mild viral rash) the new calculation expression is:\n",
      " number(number((selected(${select_symptoms_skin},'opt_none'))) or number((${ask_swelling_painful}='No')))\n",
      "The name of the combined calculate field is: d_problem_skin_unclassified\n",
      "For Extensive impetigo or ecthyma the new calculation expression is:\n",
      " number(number((${ask_ulcer_greater_4cm}='No')) or number((${ask_extensive_lesions}='Yes')))\n",
      "The name of the combined calculate field is: d_impetigo_ecthyma_extensive\n",
      "For Extensive skin infection the new calculation expression is:\n",
      " number(number((${d_scabies_infected}=1)) or number((${d_impetigo_ecthyma_extensive}=1) or (${d_cellulitis_non_febrile}=1)) or number((${d_eczema_infected}=1)) or number((${d_chickenpox_infected}=1)) or number((${d_folliculitis_facial_or_multiple}=1)))\n",
      "The name of the combined calculate field is: d_infection_skin_extensive\n",
      "For Possible schistosomiasis the new calculation expression is:\n",
      " number(number((${p_age}>=24 and ((${load_schistosomiasis_area}=1 and ((selected(${select_symptoms_other},'opt_8'))))))) or number((${load_schistosomiasis_area}=1 and ((${d_diarrhoea_persistent}=1)))))\n",
      "The name of the combined calculate field is: d_schistosomiasis_possible\n",
      "For Persistent fever the new calculation expression is:\n",
      " number(number((${ask_fever_7days_urinalysis}='Yes')) or number((${ask_fever_7days}='Yes')))\n",
      "The name of the combined calculate field is: d_fever_persistent\n",
      "For Likely viral infection the new calculation expression is:\n",
      " number(number((${ask_fever_7days}='No')) or number((${ask_fever_7days_urinalysis}='No')))\n",
      "The name of the combined calculate field is: d_infection_viral_likely\n",
      "For Presumptive malaria the new calculation expression is:\n",
      " number(number((${d_malaria_not_done}=1 and ((${d_fever_past_present}=1 and ((${d_painful_swallowing}=0 and ((${d_runny_nose_ear_inf}=0 and (((number(selected(${major_symptoms},'opt_1')) + number(selected(${major_symptoms},'opt_2')) + number(selected(${major_symptoms},'opt_3')) + number(selected(${major_symptoms},'opt_4')) + number(selected(${major_symptoms},'opt_5')) + number(selected(${major_symptoms},'opt_6')))=1 and ((${d_urinalysis}=0 and ((${d_referral_red}=0 and ((${select_result_test_malaria}='opt_3'))) or (${d_referral_red}=0 and (((${select_result_test_malaria}='opt_2'))))))))))))))))) or number((${d_malaria_not_done}=1 and ((${d_painful_swallowing}=0 and ((${d_runny_nose_ear_inf}=0 and (((number(selected(${major_symptoms},'opt_1')) + number(selected(${major_symptoms},'opt_2')) + number(selected(${major_symptoms},'opt_3')) + number(selected(${major_symptoms},'opt_4')) + number(selected(${major_symptoms},'opt_5')) + number(selected(${major_symptoms},'opt_6')))=1 and ((${d_malaria_positive}=0 and ((${d_fever_past_present}=1 and ((${d_abdominal_pain}=0 and ((${select_result_urinary_test}='opt_2') or (${select_result_urinary_test}='opt_3'))) or (${ask_constipation}='No')))))))))))))))\n",
      "The name of the combined calculate field is: d_malaria_presumptive\n"
     ]
    }
   ],
   "source": [
    "df = calcombo(df, df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82c983b",
   "metadata": {},
   "source": [
    "### put all calculates on top of df_survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bb88ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# because some are now below the treatment and then duplicates are preserved from df_tt which is wrong\n",
    "df_calc = df.loc[df['type']=='calculate']\n",
    "df.drop(df.loc[df['type']=='calculate'].index, inplace = True)\n",
    "df = pd.concat([df_calc, df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80026857",
   "metadata": {},
   "source": [
    "### make a summary xlx --> all this had to move here because in TT there was wrong relevance for the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "45f34505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will all become obsolete as the summary gets build by the scrit and DX and TT merge into one script. so \n",
    "# all this is just pasted in here. Do not improve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7b148585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_summary(df, df_choices, diagnose_id_hierarchy, summaryfile):\n",
    "    # need to reload diagnose_id_hierarchy, because the sorting here is wrong, because it is dervied from the \n",
    "    # drawing. There should be no diagnose_hierarchy in the dx flow, it makes no sense to me at all. \n",
    "    df_diagnose=df.loc[df['name'].isin(diagnose_id_hierarchy)]\n",
    "    df_diagnose['calculation']=''\n",
    "    df_diagnose['relevance']='number(${' + df['name'] + '})=1'\n",
    "    df_diagnose['appearance']='center'\n",
    "    df_diagnose['type']='note'\n",
    "    df_diagnose['label::en']='<p>' + df_diagnose['label::en'] + '</p>'\n",
    "    df_diagnose['name']=df_diagnose['name'].replace({'d_':'label_'},regex=True)\n",
    "\n",
    "    df_diagnose.index=df_diagnose.index+'label'\n",
    "    \n",
    "    intro = pd.read_excel(summaryfile).iloc[:6]\n",
    "    \n",
    "    endgroup = pd.read_excel(summaryfile).iloc[-2:]\n",
    "    \n",
    "    danger_signs = df_choices.loc[df_choices['list_name'].str.contains('select_signs') & ~df_choices['name'].str.contains('none')].copy()\n",
    "    danger_signs['relevance']='selected(${' + danger_signs['list_name'] + '},\\'' + danger_signs['name'] + '\\')'\n",
    "    danger_signs['type']='note'\n",
    "    danger_signs['name']='label_' + danger_signs['name']\n",
    "    danger_signs.index = danger_signs.index+'danger'\n",
    "    \n",
    "    df_summary = pd.concat([intro, df_diagnose, pd.read_excel(summaryfile).iloc[6:8], danger_signs, endgroup])\n",
    "    \n",
    "    df_summary.drop(columns=['list_name'], inplace = True)\n",
    "    \n",
    "    df_summary.fillna('', inplace=True)\n",
    "    \n",
    "    # make group relevance for danger sign group\n",
    "    ds_relevance = ' or '.join(danger_signs['relevance'])\n",
    "    df_summary.loc[df_summary['name']=='g_danger_signs', 'relevance'] = ds_relevance\n",
    "    \n",
    "    \n",
    "    return df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fb2b4265",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diagnose = pd.read_csv(diagnose_order)\n",
    "diagnosis_id_hierarchy = list(df_diagnose['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c00196d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7498/195263653.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_diagnose['calculation']=''\n",
      "/tmp/ipykernel_7498/195263653.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_diagnose['relevance']='number(${' + df['name'] + '})=1'\n",
      "/tmp/ipykernel_7498/195263653.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_diagnose['appearance']='center'\n",
      "/tmp/ipykernel_7498/195263653.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_diagnose['type']='note'\n",
      "/tmp/ipykernel_7498/195263653.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_diagnose['label::en']='<p>' + df_diagnose['label::en'] + '</p>'\n",
      "/tmp/ipykernel_7498/195263653.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_diagnose['name']=df_diagnose['name'].replace({'d_':'label_'},regex=True)\n"
     ]
    }
   ],
   "source": [
    "df_summary = make_summary(df, df_choices, diagnosis_id_hierarchy, p.summaryfile)\n",
    "\n",
    "# store df_summary\n",
    "import pickle\n",
    "\n",
    "with open(p.folder+'df_summary.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_summary, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b4b7f15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/anaconda3/lib/python3.9/site-packages/xlsxwriter/workbook.py:339: UserWarning: Calling close() on already closed file.\n",
      "  warn(\"Calling close() on already closed file.\")\n"
     ]
    }
   ],
   "source": [
    "# make the xlsx file! \n",
    " \n",
    "#create a Pandas Excel writer using XlsxWriter as the engine\n",
    "writer = pd.ExcelWriter(dxfile, engine='xlsxwriter')\n",
    "\n",
    "\n",
    "df.to_excel(writer, sheet_name='survey',index=False)\n",
    "df_choices.to_excel(writer, sheet_name='choices',index=False)\n",
    "df_settings.to_excel(writer, sheet_name='settings',index=False)\n",
    "\n",
    "#close the Pandas Excel writer and output the Excel file\n",
    "writer.save()\n",
    "\n",
    "# run this on a windows python instance because if not then the generated xlsx file remains open\n",
    "writer.close()\n",
    "writer.handles = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "86baef0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rafael/Documents/git/MSFeCARE/forms-clinical/ped/release20230207_context_params_facility/ped_dx.xlsx'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dxfile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
