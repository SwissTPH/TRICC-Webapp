{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06333ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import re\n",
    "import os\n",
    "import base64 # to extract images from base64 strings (as they are stored in xml files)\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "from numpy import linspace\n",
    "\n",
    "import graphtools as gt\n",
    "from loadcalculations import loadcalc\n",
    "import cleanhtml as ch #my own helper functions\n",
    "from combinecalculates import calcombo\n",
    "from treetodataframe import treetodataframe\n",
    "import caretaker_advice as ca\n",
    "import qualitychecks_pd as qcpd\n",
    "import multiheadlinesplit as mhsplit\n",
    "import odk_helpers as oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22de036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters (defined in the merge script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e81b905",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r form_id testing multiple_labels summaryfile drugsfile cafile inputfile_dx inputfile_tt \\\n",
    "dxfile ttfile output form_title input_trans updated_trans diagnose_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fac66135",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_flow=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be286ffa",
   "metadata": {},
   "source": [
    "### Parsing draw.io file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89c122e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Error reading file '../forms-clinical/ped/release20221128/ped_tt.drawio': failed to load external entity \"../forms-clinical/ped/release20221128/ped_tt.drawio\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_76099/1677088105.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputfile_tt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 'data' is a wrapper for the entire tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get the name of the highest element of the tree, put it into the variable 'root'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.//diagram'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# gets all the tabs of the document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mobjects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# all objects of all pages combined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msrc/lxml/etree.pyx\u001b[0m in \u001b[0;36mlxml.etree.parse\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._parseDocument\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._parseDocumentFromURL\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._parseDocFromFile\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._BaseParser._parseDocFromFile\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._ParserContext._handleParseResultDoc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._handleParseResult\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._raiseParseError\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Error reading file '../forms-clinical/ped/release20221128/ped_tt.drawio': failed to load external entity \"../forms-clinical/ped/release20221128/ped_tt.drawio\""
     ]
    }
   ],
   "source": [
    "data = etree.parse(inputfile_tt) # 'data' is a wrapper for the entire tree\n",
    "root = data.getroot() # get the name of the highest element of the tree, put it into the variable 'root'\n",
    "pages = root.findall('.//diagram') # gets all the tabs of the document\n",
    "\n",
    "objects = [] # all objects of all pages combined\n",
    "\n",
    "for page in pages:\n",
    "    print('Page ID:', page.attrib['id'], 'Page name:', page.attrib['name'])\n",
    "    objects_in_page = page.findall('.//mxGraphModel//mxCell')\n",
    "    objects = objects + objects_in_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88219442",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = treetodataframe(objects)\n",
    "\n",
    "# maintain compatibility with old script:\n",
    "df_raw.fillna('', inplace = True)\n",
    "df_raw['tag']=''\n",
    "df_raw.loc[df_raw['label']!='','value'] = df_raw['label']\n",
    "df_raw['xml-parent']=df_raw['parent']\n",
    "\n",
    "df_raw = df_raw[['tag', 'id', 'value', 'style', 'xml-parent',\n",
    "      'source', 'target', 'name', 'odk_type', 'min', 'max', 'required',\n",
    "      'constraint_message', 'x', 'y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d1b93a",
   "metadata": {},
   "source": [
    "### Quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1f3411",
   "metadata": {},
   "outputs": [],
   "source": [
    "qcpd.check_node_type(df_raw) # check if all objects have an odk_type\n",
    "qcpd.check_rhombus_refer(df_raw) # check if all rhombus refer to an existing node\n",
    "qcpd.check_edge_connection(df_raw) # check if all edges are well connected\n",
    "types = ['rhombus', 'select_one yesno']\n",
    "qcpd.check_edge_yesno(df_raw, types) # check if all edges leaving rhombus and select_one yesno have Yes/No"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c126d849",
   "metadata": {},
   "source": [
    "### Pause function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621ad8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify break points for the PAUSE function\n",
    "df_pause = df_raw.loc[df_raw['style'].str.contains('fillColor=#cdeb8b', na=False),['id', 'name', 'odk_type']]\n",
    "df_pause['flowtype'] = 'treatment'\n",
    "df_pause = df_pause[['id', 'name', 'odk_type', 'flowtype']]\n",
    "df_pause.to_csv('breakpoints.csv', mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8af31",
   "metadata": {},
   "source": [
    "### Constraint column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa92895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a constraint column\n",
    "df=df_raw.copy()\n",
    "df.drop(columns=['x','y'],inplace=True)\n",
    "df['constraint']=''\n",
    "df.loc[df['min']!='','constraint']='.>=' + df['min']\n",
    "df.loc[df['max']!='','constraint']=df['constraint'] + ' and .<=' + df['max']\n",
    "df.drop(columns=['min','max'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eefcb69",
   "metadata": {},
   "source": [
    "### Required fields\n",
    "if integers and decimals are not REQUIRED, the expression towards the downstream fields must be removed. See below under **Expression for each row**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['required']=='yes','required']='true()'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a98077a",
   "metadata": {},
   "source": [
    "### Clean html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aba32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove html formatting everywhere, except in 'note' and 'help-messages' (not allowed there in CHT)\n",
    "m = ~df['odk_type'].isin(['note','help-message'])\n",
    "df.loc[m,'value'] = df.loc[m,'value'].apply(lambda x: ch.remove_html(x) if x!=None else None)\n",
    "\n",
    "# clean html in 'note' and 'help-messages'\n",
    "m = df['odk_type'].isin(['note','help-message'])\n",
    "df['value'] = df['value'].apply(lambda x: ch.remove_html_value(x) if x!=None else None)\n",
    "\n",
    "# only for somalia: remove all html and split notes with multiple headings\n",
    "if form_id == 'almsom':\n",
    "    m = df['odk_type']=='note'\n",
    "    df.loc[m,'value'] = df.loc[m,'value'].apply(lambda x: ch.clean_multi_headings(x))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a5334",
   "metadata": {},
   "source": [
    "### for Somalia: split multiheadline nodes into singletons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1267cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "if form_id == 'almsom':\n",
    "    df = mhsplit.split_mh(df)\n",
    "    \n",
    "    # get rid of junk characters around the heading like 'TT Box «'\n",
    "    df['value'] = [re.search('(?<=«).*?(?=»)',i).group(0) if re.search('(?<=«).*?(?=»)',i)!=None \\\n",
    "               else i for i in df['value']]\n",
    "    \n",
    "    # clean all html from the entire df\n",
    "    df['value'] = df['value'].apply(lambda x: ch.html2plain(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc623a88",
   "metadata": {},
   "source": [
    "### Give name to diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# properly name the diagnose calculates in the TT drawing\n",
    "diagnose_hierarchy = pd.read_csv(diagnose_order)\n",
    "\n",
    "diagnose_hierarchy['map']= diagnose_hierarchy['Name'].apply(ch.clean_name) \n",
    "df['map'] = df['value'].astype(str)\n",
    "df['map'] = df['map'].apply(ch.clean_name)\n",
    "\n",
    "m = df['map'].isin(diagnose_hierarchy['map']) & (df['odk_type']=='calculate')\n",
    "dfa = df.loc[m].reset_index()\n",
    "dfa = dfa[['index','map']].merge(diagnose_hierarchy[['id','map']],how='left',on='map')\n",
    "dfa.set_index('index',inplace=True)\n",
    "dfa.rename(columns={'id':'name'},inplace=True)\n",
    "df.update(dfa)\n",
    "\n",
    "df.drop(columns=['map'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7ccaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get diagnose hierarchy expressed as id's\n",
    "dfa = df.loc[m,['id','name']].set_index('name') # slice of df containing the diagnoses (unsorted)\n",
    "dfa = dfa.reindex(list(diagnose_hierarchy['id']))['id'] # that slice, but sorted and 'nan' dropped \n",
    "# (these are diagnoses that exist in dx but not in tt, mostly non-severe ones that have no TT)\n",
    "diagnose_id_hierarchy = list(dfa.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c829a287",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df.loc[(df['name']=='d_') & (df['odk_type']=='calculate')])>0:\n",
    "    print('There were unmatched diagnose names!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc89c77",
   "metadata": {},
   "source": [
    "### Resolve name duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82068da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate names are not allowed except for rhombus, calculates (which will be combined later) and select_options \n",
    "# THIS IS BAD PRACTICE!\n",
    "#m = df.duplicated(subset=['name'],keep=False) & ~df['odk_type'].isin(['calculate', 'rhombus', 'select_option'])\n",
    "#df.loc[m, 'name']=df['name']+df['id']\n",
    "\n",
    "df.set_index('id',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2569e3d7",
   "metadata": {},
   "source": [
    "### Make dataframe with edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e695abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arrows=df.loc[(df['source']!='') & (df['target']!=''),['source','target','value']]\n",
    "\n",
    "# drop arrows from df\n",
    "df.drop(df_arrows.index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f2ef3f",
   "metadata": {},
   "source": [
    "### take into account shortcuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f068c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take into account shortcuts\n",
    "dfa = df_raw.loc[df_raw['odk_type']=='goto'].copy() # extract shortcut elements and put in a new dataframe\n",
    "dfa.loc[dfa['odk_type']=='goto','name'] = dfa.loc[dfa['odk_type']=='goto','name'].str[9:] # remove prefix\n",
    "# merge with raw-data to get the id of the exit element\n",
    "dfa = dfa.reset_index().merge(df_raw.reset_index()[['id','name']],how = 'left', on='name') \n",
    "exitmap = dict(zip(dfa['id_x'],dfa['id_y'])) # convert into a dictionnary \n",
    "df_arrows['target'] = df_arrows['target'].replace(exitmap) # replace the shortcut elements by the exit-element in df_arrows\n",
    "df.drop(df.loc[df['odk_type']=='goto'].index,inplace=True) # drop shortcuts from df_survey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2238c63b",
   "metadata": {},
   "source": [
    "### extract images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ef18b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a folder for images and other media\n",
    "\n",
    "if not(os.path.isdir('media')): # check if it exists, because if it does, error will be raised \n",
    "    # (later change to make folder complaint to CHT)\n",
    "    os.mkdir('media')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a2807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding png images that belong to container-hint-media (not included are those that belong to select_options)\n",
    "m = df['style'].str.contains(\"image/png\",na=False)\n",
    "df.loc[m,'odk_type']='png-image'+df.loc[m,'name']+'.png'\n",
    "\n",
    "# getting a dataframe with png-images only (better for joining with df later)\n",
    "# images:rows where 'xml-parent' is inside the index of rows that have the entry 'container_hint_media' in odk_type column, \n",
    "# of those rows we extract those where the 'type' column contains the substring 'png-image'\n",
    "# and of the result we just take the columns 'xml-parent', 'odk_type' and 'style'\n",
    "# 'xml-parent' is the container it belongs to and the line that will contain the info about the image\n",
    "# 'odk_type' contains also the file name .png\n",
    "# 'style' contains the actual image data\n",
    "\n",
    "df_png=df.loc[df['xml-parent'].isin(df.loc[df['odk_type']=='container_hint_media'].index) \n",
    "              & df['odk_type'].str.contains('png-image',na=False),\n",
    "              ['xml-parent','odk_type','style']] # images that are in 'containers_hint_media'\n",
    "\n",
    "# getting image data from 'style' column for all images (from containers AND select_options) and storing it to disk\n",
    "df_pngAll=df.loc[df['odk_type'].str.contains('png-image',na=False),['xml-parent','odk_type','style']]\n",
    "for index, row in df_pngAll.iterrows():\n",
    "    string = row['style'] \n",
    "    img_data=re.search('image/png,(.+?);',string).group(1) # extract image data from 'style' column using regex\n",
    "    with open('media/'+row['odk_type'], \"wb\") as fh:\n",
    "        fh.write(base64.decodebytes(img_data.encode('ascii'))) # encode image into ascii (binary) and save\n",
    "\n",
    "df_png.rename({'xml-parent':'container_id','odk_type':'image::en'},axis=1,inplace=True)\n",
    "index_delete=df_png.index\n",
    "df_png.set_index('container_id',inplace=True)\n",
    "df_png.drop('style',axis=1,inplace=True)\n",
    "\n",
    "# joinging df and df_png (this adds the media-image column to df)\n",
    "df=df.join(df_png)\n",
    "\n",
    "# remove the rows with those 'png messages' in df as they are no longer needed\n",
    "df.drop(index_delete,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a76f10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding jpeg images that belong to container-hint-media (not included are those that belong to select_options)\n",
    "df.loc[df['style'].str.contains(\"image/jpeg\",na=False),'odk_type']='jpeg-image'+df.name+'.jpeg'\n",
    "\n",
    "# getting a dataframe with png-images only (better for joining with df later)\n",
    "# images:rows where 'xml-parent' is inside the index of rows that have the entry 'container_hint_media' in odk_type column, \n",
    "# of those rows we extract those where the 'type' column contains the substring 'png-image'\n",
    "# and of the result we just take the columns 'xml-parent', 'odk_type' and 'style'\n",
    "# 'xml-parent' is the container it belongs to and the line that will contain the info about the image\n",
    "# 'odk_type' contains also the file name .png\n",
    "# 'style' contains the actual image data\n",
    "\n",
    "df_png=df.loc[df['xml-parent'].isin(df.loc[df['odk_type']=='container_hint_media'].index) \n",
    "              & df['odk_type'].str.contains('jpeg-image',na=False),\n",
    "              ['xml-parent','odk_type','style']] # images that are in 'containers_hint_media'\n",
    "\n",
    "# getting image data from 'style' column for all images (from containers AND select_options) and storing it to disk\n",
    "df_pngAll=df.loc[df['odk_type'].str.contains('jpeg-image',na=False),['xml-parent','odk_type','style']]\n",
    "for index, row in df_pngAll.iterrows():\n",
    "    string = row['style'] \n",
    "    img_data=re.search('image/jpeg,(.+?);',string).group(1) # extract image data from 'style' column using regex\n",
    "    with open('media/'+row['odk_type'], \"wb\") as fh:\n",
    "        fh.write(base64.decodebytes(img_data.encode('ascii'))) # encode image into ascii (binary) and save\n",
    "\n",
    "df_png.rename({'xml-parent':'container_id','odk_type':'image::en'},axis=1,inplace=True)\n",
    "index_delete=df_png.index\n",
    "df_png.set_index('container_id',inplace=True)\n",
    "df_png.drop('style',axis=1,inplace=True)\n",
    "\n",
    "# joinging df and df_png (this adds the media-image column to df)\n",
    "df.update(df_png)\n",
    "\n",
    "# remove the rows with those 'png messages' in df as they are no longer needed\n",
    "df.drop(index_delete,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834ca0b3",
   "metadata": {},
   "source": [
    "### Create and populate 'help' & 'hint' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2345d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in ['hint-message', 'help-message']:\n",
    "\n",
    "    dfa=df_raw.loc[df_raw['odk_type']==s,['xml-parent','value']] # dataframe with help-fields / hint-fields only\n",
    "    drop_index = df_raw.loc[df_raw['odk_type']==s, 'id']\n",
    "    dfa.set_index('xml-parent', inplace = True) # in order to join dfa and df on index\n",
    "    sa = s[:-8]+'::en'\n",
    "    dfa.rename(columns = {'value':sa}, inplace = True) \n",
    "    df=df.join(dfa) # this adds the help message column to df\n",
    "    df.drop(drop_index, inplace = True) # remove 'help' rows from df (that data is now in the 'help' column)\n",
    "    \n",
    "df.fillna('', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a9b71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe that will be needed later to replace sources in df_arrows which are inside a container, by the container itself\n",
    "\n",
    "df_new_arrow_sources = df.loc[df['xml-parent'].isin(df.loc[df.odk_type=='container_hint_media'].index) \n",
    "                              | df['xml-parent'].isin(df.loc[df.odk_type=='container_page'].index),['xml-parent','odk_type']]\n",
    "df_new_arrow_sources.rename({'xml-parent':'container_id','odk_type':'odk_type_of_content'},axis=1,inplace=True)\n",
    "\n",
    "# add also the type of the container (page or hint-image)\n",
    "df_new_arrow_sources = df_new_arrow_sources.merge(df[['odk_type']],how='left',left_on='container_id',right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65748438",
   "metadata": {},
   "source": [
    "### replace 'container_hint_media' labels with those of their children & drop children from df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65587fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_ids = df_raw[df_raw['odk_type']=='container_hint_media']['id']\n",
    "m = df_raw['xml-parent'].isin(container_ids) & ~df_raw['style'].str.contains('image',na=False) & ~df_raw['odk_type'].isin(['hint-message', 'help-message'])\n",
    "label_ids = list(df_raw[m]['id']) # used for dropping the labels from df after uploading info to container rows\n",
    "df_label = df_raw.loc[m, ['xml-parent','value','odk_type','name', 'id']] # all the label-children of containers \n",
    "df_label.set_index('xml-parent', inplace=True)\n",
    "# ATTENTION! df_raw still has duplicate names -> duplicates in df_label['name'], so fix it now:\n",
    "df_label.loc[df_label.duplicated(subset = ['name']), 'name'] = df_label['name'] + df_label['id']\n",
    "df.update(df_label) # update the containers' 'value', 'odk_type' and 'name'\n",
    "\n",
    "df.drop(label_ids, inplace = True) # drop the children from df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba09122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for connectors where the source is inside a container-hint-media, replace the source with the container itself\n",
    "df_hint_media_objects = df_new_arrow_sources.loc[df_new_arrow_sources['odk_type']=='container_hint_media']\n",
    "df_arrows = df_arrows.merge(df_hint_media_objects,how='left',left_on='source',right_index=True)\n",
    "df_arrows.rename(columns={'odk_type':'container_type'},inplace=True)\n",
    "m=(df_arrows['container_type']=='container_hint_media')\n",
    "df_arrows.loc[m,'source']=df_arrows.loc[m,'container_id']\n",
    "df_arrows.loc[m,'source_type']=df_arrows.loc[m,'odk_type_of_content']\n",
    "df_arrows.drop(columns=['container_id','odk_type_of_content','container_type'],inplace=True)\n",
    "df_arrows.fillna('',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93af9bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a dataframe with all choice options for all valueSets (choices tab)\n",
    "# all elements whose 'xml-parent' is the 'id' of elements that have 'select_xxx' in type\n",
    "# these are all options (elements of valuesets)\n",
    "df_choices=df.loc[df['odk_type']=='select_option']\n",
    "df_choices=df_choices.merge(df[['name','odk_type']],how='left',left_on='xml-parent',right_index=True)\n",
    "df.drop(index_delete,inplace=True) # drop the labels from df (now you can, cause you have them in df_choices)\n",
    "df_choices=df_choices[['name_y','name_x','value','odk_type_y']]\n",
    "\n",
    "# info: the 'odk_type' is kept because it will be necessary for making the logic (relevance column)\n",
    "df_choices.rename({'name_y':'list_name','name_x':'name','value':'label::en','odk_type_y':'odk_type'},axis=1,inplace=True)\n",
    "\n",
    "# remove the rows with 'choices' in df as they are no longer needed\n",
    "df.drop(df_choices.index,inplace=True)\n",
    "\n",
    "# make a dataframe that contains only remaining image objects (those that belong to options)\n",
    "df_png = df.loc[df['odk_type'].str.contains('-image',na=False),'odk_type'].to_frame()\n",
    "# drop the select_option images from df\n",
    "df.drop(df_png.index)\n",
    "# merge with df_arrows to add the \n",
    "df_png = df_png.merge(df_arrows[['source','target']],how='left',left_index=True,right_on='source')\n",
    "df_png.rename(columns={'odk_type':'image::en'},inplace=True)\n",
    "# add the image name to df_choices\n",
    "df_choices = df_choices.reset_index().merge(df_png[['image::en','target']],\\\n",
    "                                            how='left',left_on='id',right_on='target').set_index('id')\n",
    "# drop the target column\n",
    "df_choices.drop(columns=['target'],inplace=True)\n",
    "\n",
    "# drop the remaining unspecified objects (pure xml formating related elements or drawing artefacts) \n",
    "df.drop(df.loc[df.value==''].index,inplace=True)\n",
    "\n",
    "# add rows for yesno\n",
    "yes=['yesno','Yes','Yes','select_one','']\n",
    "no=['yesno','No','No','select_one','']\n",
    "df_choices.loc['zzz_yes']=yes\n",
    "df_choices.loc['zzz_no']=no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d588307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e2ea8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing df_arrows for logic part:\n",
    "\n",
    "# rename index of df_arrows to reduce confusion\n",
    "df_arrows.index.rename('Arrow ID',inplace=True)\n",
    "\n",
    "# make a logical expression for each arrow\n",
    "\n",
    "# add names of the source from df (for the case when the source is NOT a select_xxx) (names are the odk id's)\n",
    "# the value is only needed for the rhombus\n",
    "\n",
    "'''\n",
    "First we merge with df and then again with df_choices. The reason for that: at this stage, \n",
    "the arrows originate from select_xxx options (opt1,opt2,...), but do not point to them. \n",
    "However, at a later stage, those arrows are modified so they originate from the select_xxx itself. If that step was done \n",
    "before, we would not need to have to merge twice here. When improving the form builder, consider changing this. \n",
    "'''\n",
    "# merging with df to get the odk_type\n",
    "df_arrows=df_arrows.merge(df[['name','odk_type']],how='left',left_on='source',right_index=True)\n",
    "# moving the type of the source into the column 'source_type'\n",
    "df_arrows.loc[df_arrows['source_type']=='','source_type']=df_arrows.loc[df_arrows['source_type']=='','odk_type']\n",
    "# droping the 'odk_type' column, it is no longer needed\n",
    "df_arrows.drop(columns=['odk_type'],inplace=True)\n",
    "df_arrows.fillna('',inplace=True)\n",
    "\n",
    "# merging with df_choices to get the odk_type for when the source is a select_xxx\n",
    "df_arrows=df_arrows.merge(df_choices[['list_name','name','odk_type']],how='left',left_on='source',right_index=True)\n",
    "# as before for df, moving the type of the source into the column 'source_type'\n",
    "df_arrows.loc[df_arrows['source_type']=='','source_type']=df_arrows.loc[df_arrows['source_type']=='','odk_type']\n",
    "df_arrows.fillna('',inplace=True)\n",
    "\n",
    "# merge names from df and df_choices into one column\n",
    "df_arrows['source_name']=df_arrows['name_x']+df_arrows['list_name']\n",
    "df_arrows.drop(['name_x','list_name','odk_type'],axis=1,inplace=True)\n",
    "df_arrows.rename(columns={'name_y':'select_option'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f8f44",
   "metadata": {},
   "source": [
    "### Expression for each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6977312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arrows['expression']=''\n",
    "\n",
    "# add connectors to virtual objects (loaded objects)\n",
    "\n",
    "# expression for yes no questions\n",
    "df_arrows.loc[df_arrows['source_type']=='select_one yesno','expression'] = '${'+df_arrows['source_name'] + '}=' + '\\'' + df_arrows.value + '\\''\n",
    "\n",
    "# expression for integers and decimals\n",
    "#df_arrows.loc[(df_arrows['source_type']=='integer') | (df_arrows['source_type']=='decimal'),'expression'] = '${'+df_arrows['source_name'] + '}!=' + '\\'\\''\n",
    "# for integers and decimals that are NOT required, the expression must be removed:\n",
    "#m1 = df_arrows['source_type'].isin(['integer', 'decimal'])\n",
    "#m2 = df_raw['odk_type'].isin(['integer', 'decimal'])\n",
    "#df_arrow_int = df_arrows.loc[m1].reset_index().merge(df_raw.loc[m2, ['name', 'required']], how = 'left', left_on = 'source_name', right_on = 'name').set_index('Arrow ID')\n",
    "# merge with df_raw to get the 'required'\n",
    "#rowIDs = df_arrow_int.loc[df_arrow_int['required']=='no'].index\n",
    "#df_arrows.loc[rowIDs, 'expression']=''\n",
    "\n",
    "# expression for text-entry fields\n",
    "df_arrows.loc[df_arrows['source_type']=='text','expression'] = '${'+df_arrows['source_name'] + '}!=' + '\\'\\''\n",
    "\n",
    "# expression for all the other select_one\n",
    "df_arrows.loc[df_arrows['source_type']=='select_one','expression'] = '${'+df_arrows['source_name'] + '}=' + '\\'' + df_arrows['select_option'] + '\\''\n",
    "\n",
    "# expression for select_multiple\n",
    "df_arrows.loc[df_arrows['source_type']=='select_multiple','expression'] = 'selected(${'+df_arrows['source_name'] + '},\\'' + df_arrows['select_option'] + '\\')'\n",
    "\n",
    "# expression for source being a calculate\n",
    "df_arrows.loc[df_arrows['source_type']=='calculate','expression'] = '${'+df_arrows['source_name'] + '}=1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70addb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expression for target being a count---> in this case the expression depends not on the source but on the target!\n",
    "counters=df.loc[df['odk_type']=='count'].index\n",
    "m = df_arrows['target'].isin(df.loc[df['odk_type']=='count'].index) # mask for connectors that point to 'count' objects\n",
    "df_arrows.loc[m,'expression'] = 'number(' + df_arrows.loc[m,'expression'] + ')'\n",
    "\n",
    "# add arrow weight to counter\n",
    "m = df_arrows['value'].isin(['1','2','3']) & (df_arrows['target'].isin(df.loc[df['odk_type']=='count'].index))\n",
    "df_arrows.loc[m,'expression'] =  df_arrows.loc[m,'value'] + ' * ' + df_arrows.loc[m,'expression']\n",
    "\n",
    "# for counters you must combine the expression of all icoming arrows into the one expression of that counter. \n",
    "# from there on, a rhombus, referring to a counter can lookup the entire expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afe505b",
   "metadata": {},
   "source": [
    "### Expression for rhombus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b06bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = df_arrows['source_type']=='rhombus'\n",
    "dfa = df_arrows.loc[m].copy()\n",
    "# remove prefix 'stored_'\n",
    "# ATTENTION! There is a BUG in pandas, replace(.... inplace = True) is not working!\n",
    "dfa.loc[m, 'source_name'] = dfa.loc[m, 'source_name'].replace(r'^stored_', r'', regex = True)\n",
    "\n",
    "\n",
    "# look up the odk_type that the rhombus is refering to\n",
    "dfa = dfa.reset_index().merge(df[['odk_type','name']],how='left',left_on='source_name',right_on='name').set_index('Arrow ID')\n",
    "# get rid of the 'name' column (was just needed for merging) and rename 'odk_type' column, to avoid confusion\n",
    "dfa.drop('name',axis=1,inplace=True)\n",
    "dfa.rename(columns={'odk_type':'rhombus_refer_to_odk_type'},inplace=True)\n",
    "\n",
    "# look up the value of the rhombus, it contains info about the logic\n",
    "dfa = dfa.merge(df[['value']],how='left',left_on='source',right_index=True)\n",
    "dfa.rename(columns={'value_x':'value','value_y':'value_of_rhombus'},inplace=True)\n",
    "\n",
    "# set all 'NaN' to empty strings\n",
    "df_arrows=df_arrows.fillna('')\n",
    "df_arrows['rhombus_refer_to_odk_type']=''\n",
    "df_arrows['value_of_rhombus']=''\n",
    "\n",
    "df_arrows.update(dfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e59fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when rhombus refers to a an integer or decimal\n",
    "m = (df_arrows['source_type']=='rhombus') & (df_arrows['rhombus_refer_to_odk_type'].isin(['integer','decimal']))\n",
    "# only keep what comes after <,= or >\n",
    "df_arrows.loc[m,'value_of_rhombus'] = df_arrows.loc[m,'value_of_rhombus'].str.replace(r'^[^<=>]+','',regex=True)\n",
    "# remove the '?' at the end\n",
    "df_arrows.loc[m,'value_of_rhombus'] = df_arrows.loc[m,'value_of_rhombus'].str.replace('?','',regex=False)\n",
    "df_arrows.loc[m,'expression'] = '${'+df_arrows['source_name'] + '}' + df_arrows['value_of_rhombus']\n",
    "df_arrows.loc[m & (df_arrows['value']=='No')] = df_arrows.loc[m & (df_arrows['value']=='No')].replace({'<=':'>','>=':'<','<':'>=','>':'<='},regex=True)\n",
    "\n",
    "# when rhombus refers to a select_one yesno\n",
    "m = (df_arrows['source_type']=='rhombus') & (df_arrows['rhombus_refer_to_odk_type']=='select_one yesno')\n",
    "df_arrows.loc[m,'expression'] = '${'+df_arrows['source_name'] + '}=' + '\\'' + df_arrows.value + '\\''\n",
    "\n",
    "# now the real select_ones:\n",
    "m = (df_arrows['source_type']=='rhombus') & df_arrows['rhombus_refer_to_odk_type'].isin(['select_one', 'select_multiple'])\n",
    "df_arrows.loc[m,'value_of_rhombus'] = df_arrows.loc[m,'value_of_rhombus'].str.extract(r'\\[(.*?)\\]',expand=False)\n",
    "# merge again with df_choices to get the 'name' of the selected option (also needed for select_multiple!)\n",
    "df_arrows = df_arrows.reset_index().merge(df_choices[['list_name','name','label::en']], \\\n",
    "                how='left',left_on=['source_name','value_of_rhombus'],right_on=['list_name','label::en']).set_index('Arrow ID')\n",
    "# when the outgoing arrow is YES (means that what is in RHOMBUS is TRUE)\n",
    "df_arrows.loc[m & (df_arrows['value']=='Yes'),'expression'] =  '${'+df_arrows['source_name'] + '}=' + '\\'' + df_arrows['name'] + '\\''\n",
    "# when the outgoing arrow is NO (means that what is in RHOMBUS is FALSE)\n",
    "df_arrows.loc[m & (df_arrows['value']=='No'),'expression'] =  '${'+df_arrows['source_name'] + '}!=' + '\\'' + df_arrows['name'] + '\\''\n",
    "\n",
    "# when rhombus refers to select_multiple\n",
    "# when the outgoing arrow is YES (means that what is in RHOMBUS is TRUE)\n",
    "df_arrows.loc[m & (df_arrows['value']=='Yes'),'expression'] = 'selected(${'+df_arrows['source_name'] + '},\\'' + df_arrows['name'] + '\\')'\n",
    "# when the outgoing arrow is NO (means that what is in RHOMBUS is FALSE)\n",
    "df_arrows.loc[m & (df_arrows['value']=='No'),'expression'] = 'not(selected(${'+df_arrows['source_name'] + '},\\'' + df_arrows['name'] + '\\'))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afbe768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when rhombus refers to calculate\n",
    "m = (df_arrows['source_type']=='rhombus') & (df_arrows['rhombus_refer_to_odk_type']=='calculate')\n",
    "# when the outgoing arrow is YES (means that what is in RHOMBUS is TRUE)\n",
    "df_arrows.loc[m & (df_arrows['value']=='Yes'),'expression'] = '${'+df_arrows['source_name'] + '}=1'\n",
    "# when the outgoing arrow is NO (means that what is in RHOMBUS is False)\n",
    "df_arrows.loc[m & (df_arrows['value']=='No'),'expression'] = '${'+df_arrows['source_name'] + '}=0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba686b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when rhombus refers to a count (in this case we must combine all 'expressions' of the incoming arrows into the count object \n",
    "# with ' + ') and put the result into the 'expression' of the rhombus that is refering to it\n",
    "m = (df_arrows['source_type']=='rhombus') & (df_arrows['rhombus_refer_to_odk_type']=='count')\n",
    "df_arrows.loc[m,'value_of_rhombus'] = df_arrows.loc[m,'value_of_rhombus'].str.replace(r'^[^<=>]+','',regex=True) # only keep what comes after <,= or >\n",
    "df_arrows.loc[m,'value_of_rhombus'] = df_arrows.loc[m,'value_of_rhombus'].str.replace('?','',regex=False) # remove the '?' at the end\n",
    "\n",
    "# new mask to get the df_arrows of all connectors that point to counters\n",
    "m1 = df_arrows['target'].isin(df.loc[df['odk_type']=='count'].index) # mask for connectors that point to 'count' objects\n",
    "gk = df_arrows.loc[m1].groupby('target') # group them by counters\n",
    "\n",
    "for elem, group in gk:\n",
    "    # for each counter (elem), combine the expressions of all incoming arrows into a single one, concatenated with +\n",
    "    full_expression=' + '.join(filter(None,group['expression']))\n",
    "    # put result into brackets, because comparison is executed BEFORE +\n",
    "    full_expression = '(' + full_expression + ')'\n",
    "    \n",
    "    # lookup the 'name' of the counter in df, based on the id = target\n",
    "    counter_name = df.loc[elem,'name']\n",
    "    \n",
    "    # check in df_arrows where the source_name is 'counter_name'\n",
    "    # for the 'No' arrow we invert >, < and = of 'value of rhombus'\n",
    "    m2 = (df_arrows['source_name']==counter_name) & (df_arrows['value']=='No')\n",
    "    df_arrows.loc[m & m2,'value_of_rhombus'] = df_arrows.loc[m & m2,'value_of_rhombus'].replace({'<=':'>','>=':'<','<':'>=','>':'<=','=':'!=','!=':'='},regex=True)\n",
    "    df_arrows.loc[m & (df_arrows['source_name']==counter_name),'expression'] = full_expression + df_arrows['value_of_rhombus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c380b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also drop the arrows that point to counters\n",
    "df_arrows = df_arrows[df_arrows['target'].isin(df.loc[df['odk_type']!='count'].index)]\n",
    "\n",
    "# drop no longer necessary columns\n",
    "df_arrows.drop(columns=['value','value_of_rhombus','source_name','rhombus_refer_to_odk_type','list_name','label::en','name'],inplace=True)\n",
    "\n",
    "# also drop count objects from df, they are no longer needed\n",
    "df.drop(df[df['odk_type']=='count'].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fd4012",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''A rhombus can refer to a field that is not in the drawing. For instance, in the TT flow, where values like fever are used\n",
    "but not calculated. Or in CHT, when patient info or hospital info is loaded into the input section. \n",
    "For this, the symbols are drawn in the beginning of the flow, pointing to the note field 'Load Data'. \n",
    "Once this is done, it is handled correctly by the script and they get included. '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d05edad",
   "metadata": {},
   "source": [
    "### Change sources that are 'select_options' to the 'select_xxx' itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d964e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the select_xxx for each select_option:\n",
    "dfa = df_raw.loc[df_raw['odk_type']=='select_option',['id', 'xml-parent']]\n",
    "# some select_xxx are in a container-hint-media, their ids have been replaced with the ids of the containers\n",
    "# therefore lookup the xml-parent of the select_xxx:\n",
    "dfa = dfa.merge(df_raw[['id', 'xml-parent']], how = 'left', left_on='xml-parent', right_on = 'id', suffixes=('', '_y'))\n",
    "# and if it is a container-hint-media, replace the 'xml-parent' of the select_option with the id of the container\n",
    "container_ids = list(df_raw.loc[df_raw['odk_type']=='container_hint_media', 'id'])\n",
    "m = dfa['xml-parent_y'].isin(container_ids)\n",
    "dfa.loc[m, 'xml-parent'] = dfa.loc[m, 'xml-parent_y']\n",
    "\n",
    "# make a dictionnary for replacing sources in df_arrows\n",
    "d = dict(zip(dfa.iloc[:,0], dfa.iloc[:,1]))\n",
    "df_arrows['source'].replace(d, inplace = True) # replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a9ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for connectors where the source is inside a container-hint-media, replace the source with the container itself\n",
    "df_arrows = df_arrows.merge(df_new_arrow_sources,how='left',left_on='source',right_index=True)\n",
    "df_arrows.fillna('',inplace=True)\n",
    "df_arrows.rename(columns={'odk_type':'container_type'},inplace=True)\n",
    "m=(df_arrows['container_type']=='container_hint_media')\n",
    "df_arrows.loc[m,'source']=df_arrows.loc[m,'container_id']\n",
    "df_arrows.loc[m,'source_type']=df_arrows.loc[m,'odk_type_of_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4239fad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get container_ids of pages\n",
    "container_ids = df_arrows.loc[df_arrows['container_type']=='container_page','container_id'].unique()\n",
    "\n",
    "# the ids of objects which are inside the page - containers\n",
    "page_objects = df.loc[df['xml-parent'].isin(container_ids)].index\n",
    "\n",
    "# get those page_objects which are the starting point of the flow INSIDE the page\n",
    "page_starts = page_objects[~page_objects.isin(df_arrows['target'])]\n",
    "\n",
    "# get the page_starts that are a rhombus (needed for later)\n",
    "page_starts_rhombus = df.loc[page_starts].loc[df['odk_type']=='rhombus'].index\n",
    "\n",
    "# get the page_objects where all objects in a single page are notes (needed for later)\n",
    "\n",
    "# get page_start - container_id pairs\n",
    "dfnew_connectors = df.loc[page_starts,['xml-parent']].reset_index().rename(columns={'id':'target','xml-parent':'source'})\n",
    "\n",
    "# add missing columns\n",
    "dfnew_connectors = dfnew_connectors.reindex(columns=['source','target','source_type','expression','container_id','container_type'])\n",
    "dfnew_connectors['source_type']='page'\n",
    "dfnew_connectors.fillna('',inplace=True)\n",
    "\n",
    "# concat that to df_arrows\n",
    "df_arrows = pd.concat([df_arrows,dfnew_connectors])\n",
    "\n",
    "# adding 'target_type' to df_arrows\n",
    "df_arrows = df_arrows.merge(df['odk_type'],how='left',left_on='target',right_index=True)\n",
    "df_arrows.rename(columns={'odk_type':'target_type'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486d7ffa",
   "metadata": {},
   "source": [
    "### Build DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eac45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a directed graph \n",
    "dag = nx.from_pandas_edgelist(df_arrows, source='source', target='target', create_using=nx.DiGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f29687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the new graph is a DAG -> should evaluate to True\n",
    "if not nx.is_directed_acyclic_graph(dag):\n",
    "    print('Your graph has loops. Please open them and repeat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2bdef1",
   "metadata": {},
   "source": [
    "### Build relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bacffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is necessary because there are pages that contain 'note' fields only. \n",
    "# In this case notes that point ouf of the page, have no 'expression'. This interrupts the flow. \n",
    "# The solution is to give those 'notes' as expression the 'relevant' of the page\n",
    "\n",
    "df_pageObjects = df.loc[df['xml-parent'].isin(df.loc[df['odk_type']=='container_page'].index)]\n",
    "\n",
    "# get ids of pages that ONLY contain 'notes'\n",
    "pure_note_pages=[]\n",
    "gk = df_pageObjects.groupby('xml-parent')\n",
    "for elem,frame in gk: \n",
    "    if len(frame.index) == len(frame.loc[frame['odk_type']=='note']):\n",
    "        pure_note_pages.append(elem)\n",
    "\n",
    "# get all the 'notes' that point out pages:\n",
    "df_notes_out_pages = df_arrows.loc[df_arrows['source'].isin(df_pageObjects.index) & \\\n",
    "                                    ~df_arrows['target'].isin(df_pageObjects.index) & (df_arrows['source_type']=='note')]\n",
    "\n",
    "# among those get those notes that belong to 'pure_note_pages' - these are the notes you are looking for\n",
    "df_notes_outof_pure_notes_pages = df_notes_out_pages.loc[df_notes_out_pages['container_id'].isin(pure_note_pages)]\n",
    "df_notes_outof_pure_notes_pages = df_notes_outof_pure_notes_pages[['source','container_id']]\n",
    "df_notes_outof_pure_notes_pages.set_index('source',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13832d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the objects \n",
    "# this is not the correct sorting for the flow, but any topological sort is good for making 'relevance'\n",
    "# the real sorting for the form is done later, after nodes have been combined\n",
    "node_hierarchy=list(nx.lexicographical_topological_sort(dag)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ba0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['relevant']=''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127873aa",
   "metadata": {},
   "source": [
    "Building the logic: \n",
    "1. It must be done for each object independently, not for all at once, so there is a for loop\n",
    "2. Start on the very top and go down the tree. This is the reason why we have topologically sorted df in the previous step\n",
    "3. For each object lookup all sources in df_arrows (get all rows from df_arrows where the object is the target). \n",
    "4. Each source -> target arrow has a logic expression and the entire 'relevant' of the target is just the logic expressions of all incoming arrows, combined with a OR. \n",
    "5. A particular attention must be paid when a source is a 'note'. For those sources the 'expression' is empty. That is because there is no decision taken for those objects. A note is just an info to the user and forward to the next field. There is also only one arrow coming out from a note. In this case we must use the relevant of the 'note' and 'calculate' source itself as the expression of note -> target. This would also be the case for 'calculate' objects, but their 'expression' has been populated already. If we do not do that, then the target would pop up independently of the 'note/calculate' condition. That would be wrong. Therefore, in df_source, the 'expression' for 'note' and 'calculate' is the 'relevant' of those sources. To get those into df_sources, we merge it with df accordingly. Therefore it is also important to do the logic from top to bottom, to assure that the relevant of the previous objects has already been done. \n",
    "6. Another particular interest is for rhombus (previously entered data). Here we also need the relevant of the rhombus \n",
    "    itself, because it must be combined with the expresion by an AND. The rhombus itself is not seen to the user, \n",
    "    so the logic depends on his relevant. For the terms to be executed in the right order, the 'relevant' must be put \n",
    "    into brackets first. \n",
    "7. After those steps we have a df_sources dataframe where the 'expression' is correct for each of the arrows (each row). \n",
    "    As said in (4) they are combined with OR and written into the 'relevant' of the object we are looking at. \n",
    "8. Another major problem are pages that contain ONLY notes. As objects inside a page automatically inherit the relevant \n",
    "    of the page itself, their expression is entirely empty. The exit not then points to a target outside the page and \n",
    "    has no expression at all. The following object would then always be displayed \n",
    "    (or never, if there are other arrows pointing to)\n",
    "    To deal with this we identify all those objects (groups that contain only notes and )\n",
    "9. Another problem is when the first object in a page is a rhombus. It also gets no relevant generated. As a consequence, \n",
    "    we would get just the expression with 'and ()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bacef0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for elem in node_hierarchy:\n",
    "    # df_sources: dataframe that contains all connections pointing to the object 'elem'\n",
    "    df_sources = df_arrows.loc[df_arrows['target']==elem,['source','source_type','expression']]\n",
    "    # pulling the relevant of the sources into df_sources. This corresponds to the logic to each elem. \n",
    "    # 'xml-parent' is needed for rhombus at beginning of a page\n",
    "    df_sources = df_sources.merge(df[['relevant','xml-parent']],how='left',left_on='source',right_index=True) \n",
    "\n",
    "    # when the source is a rhombus and it's relevant IS empty and the rhombus is on a page\n",
    "    # you have to combine the expression with the relevant of the page\n",
    "    # first merge with df again to the the relevant of the page\n",
    "    df_sources = df_sources.merge(df[['relevant']],how='left',left_on='xml-parent',right_index=True,suffixes=('', '_page'))\n",
    "    m=df_sources['source_type'].isin(['rhombus']) & (df_sources['relevant']=='') & df_sources['xml-parent'].isin(container_ids)\n",
    "    df_sources.loc[m,'expression'] = df_sources.loc[m,'expression'] + ' and (' + df_sources.loc[m,'relevant_page'] + ')'    \n",
    "    \n",
    "    # when the source is a rhombus and it's relevant is NOT empty, you have to combine both with AND\n",
    "    m=df_sources['source_type'].isin(['rhombus']) & (df_sources['relevant']!='')\n",
    "    df_sources.loc[m,'expression'] = df_sources.loc[m,'expression'] + ' and (' + df_sources.loc[m,'relevant'] + ')'\n",
    "    \n",
    "    # when the source is a note, integer or decimal, just take its relevant and put it into expression\n",
    "    m=df_sources['source_type'].isin(['note', 'integer', 'decimal'])\n",
    "    df_sources.loc[m,'expression'] = df_sources.loc[m,'relevant']    \n",
    "\n",
    "    # when the source is a note that is pointing out of a page that only contains 'notes' use the page relevant as \n",
    "    # its expression \n",
    "    m=df_sources['source'].isin(df_notes_outof_pure_notes_pages.index) # Mask to get pure note elements\n",
    "    df_sources.loc[m,'expression'] = df_sources.loc[m,'relevant_page']\n",
    "    #page_id = df.loc[df_sources.loc[m,'source'],'xml-parent'] # # get the page_ids of the pages the elments are in\n",
    "    # df_sources.loc[m,'expression'] = df.loc[page_id,'relevant'].to_list()\n",
    "    \n",
    "    if df.loc[elem,'odk_type']!='count':\n",
    "        df.loc[elem,'relevant'] = ' or '.join(filter(None,df_sources['expression']))\n",
    "        if '( and (' in df.loc[elem,'relevant'] or '( or (' in df.loc[elem,'relevant'] or '()' in df.loc[elem,'relevant']:\n",
    "            print(elem, 'error!')\n",
    "            print(df.loc[elem,'relevant'])\n",
    "    else:\n",
    "        # for counters the joining is number + number\n",
    "        df.loc[elem,'relevant'] = ' + '.join(filter(None,df_sources['expression'])) \n",
    "        if '( and (' in df.loc[elem,'relevant'] or '( or (' in df.loc[elem,'relevant'] or '()' in df.loc[elem,'relevant']:\n",
    "            print(elem, 'error!')\n",
    "            print(df.loc[elem,'relevant'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf9a94",
   "metadata": {},
   "source": [
    "### Remove rhombus nodes from graph and df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406aa52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking out rhombus objects of the graph\n",
    "rhombus_id = df.loc[df['odk_type']=='rhombus'].index\n",
    "new_edges=list(dag.edges)\n",
    "\n",
    "for node in rhombus_id: \n",
    "    new_edges = gt.cut_node(new_edges,node)\n",
    "    \n",
    "dag = nx.from_edgelist(new_edges, create_using=nx.DiGraph)\n",
    "df.drop(df.loc[df['odk_type']=='rhombus'].index, inplace=True) # dropping rhombus from df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfeb1e3",
   "metadata": {},
   "source": [
    "### write node attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297fe7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write node 'relevant', 'names', 'types' and 'text' as attribute to graph\n",
    "n = dict(zip(df.index, df['relevant']))\n",
    "nx.set_node_attributes(dag, n, name = 'relevant')\n",
    "n = dict(zip(df.index, df['name']))\n",
    "nx.set_node_attributes(dag, n, name = 'name')\n",
    "n = dict(zip(df_raw['id'], df_raw['odk_type']))\n",
    "nx.set_node_attributes(dag, n, name = 'type')\n",
    "n = dict(zip(df.index, df['value']))\n",
    "nx.set_node_attributes(dag, n, name = 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66dc5aa",
   "metadata": {},
   "source": [
    "### make a select-multiple for diagnosis and connect it with the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194226a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting the diagnosis to a select multiple and to the dataloader should happen\n",
    "# before the relevant is built!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d0956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a 'select_multiple diagnosis' at the beginning of the graph\n",
    "# connect diagnosis - calculates to it\n",
    "# add a relevant to the diagnosis: it is needed to deal with multiple entry nodes in the TT diagram\n",
    "# :it will also be kept for a standalone TT where the user selects the diagnosis\n",
    "n = 'select_diagnosis'\n",
    "n_attrib = {'name':'select_diagnosis', 'relevant':'', 'type':'select_multiple', 'text':'Select diagnosis'}\n",
    "dag = gt.add_calculate_selector(dag, n, n_attrib, diagnose_id_hierarchy)\n",
    "# add the diagnosis to df_choices\n",
    "n_diagnoses = [(i, dag.nodes[i]['name'], dag.nodes[i]['text']) for i in diagnose_id_hierarchy]\n",
    "list_name = 'select_diagnosis'\n",
    "df_choices = oh.add_calculate_to_choices(dag, n_diagnoses, list_name, df_choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7785493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect the dataloader with the 'select_multiple diagnosis' node\n",
    "# this insures that the dataloader elements show up on top of the form and not at the bottom\n",
    "id_dataloader = df_raw.loc[df_raw['value']=='Load Data', 'id'].iloc[0] # get ID of the dataloader\n",
    "dag.add_edge(id_dataloader, 'select_diagnosis') # connect dataloader to select_diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a0bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a 'select_dataload' multiple choice that has the calculates of the dataloader as select_options\n",
    "# this will allow to set them on startup\n",
    "# adapt also the relevant of those calculates so they react to the select_multiple dataloader \n",
    "n = 'select_dataload'\n",
    "n_attrib={'name':'select_dataload', 'relevant':'', 'type':'select_multiple', 'text':'Select previous data'}\n",
    "dataloader_calculates = [i for (i,j) in dag.in_edges(id_dataloader) if dag.nodes[i]['type']=='calculate']\n",
    "dag = gt.add_calculate_selector(dag, n, n_attrib, dataloader_calculates)\n",
    "# add the diagnosis to df_choices\n",
    "list_name = 'select_dataload'\n",
    "n_dataload_calculates = [(i, dag.nodes[i]['name'], dag.nodes[i]['text']) for i in dataloader_calculates]\n",
    "df_choices = oh.add_calculate_to_choices(dag, n_dataload_calculates, list_name, df_choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbd195f",
   "metadata": {},
   "source": [
    "### Calculate the longest path and select_option hierarchy for topological sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7150cee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as indicated in the conceptual document, this step must be done before contracting the nodes, in oder\n",
    "# to avoid that diagnosis branches are mixed up after contracted nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc406ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. diagnosis and data_loader calculates can now be selected in a select_multiple. \n",
    "# If the right order given, the graph will be correctly sorted\n",
    "# for this to work we simulate a select_option ordering of the diagnosis\n",
    "# 2. To this we concat the other select_options of the TT graph (if existant)\n",
    "# -> we have a complete TT graph that can be sorted by the same functions as the DX graph\n",
    "a = linspace((len(diagnose_id_hierarchy)+1)/100, 0.01, num=len(diagnose_id_hierarchy)+1)\n",
    "d = dict(zip(diagnose_id_hierarchy, a))  # diagnosis priority \n",
    "opt_prio = gt.hierarchy_select_options(df_raw) # hierarchy of select_options in the form\n",
    "opt_prio = d | opt_prio # combine both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff8884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a graph entry point (typically a node pointing to the dataloader)\n",
    "rootelement = gt.get_graph_entry_point(dag, df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e577139",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = gt.get_longest_path_lengths(dag, rootelement, opt_prio, df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e78066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign distance (longest_path_length) to the nodes in dag\n",
    "nx.set_node_attributes(dag, dist, name = 'distance_from_root')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94da723",
   "metadata": {},
   "source": [
    "### Contract nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e664ba57",
   "metadata": {},
   "source": [
    "As you combine two duplicates, you must add the 'relevance' of the 'predecessor' node to the relevance of the successor node before contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf57494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['odk_type']==''].index, inplace=True) # drop elements that do not belong to the form\n",
    "df['filename']=df['value'].apply(ch.clean_name) # clean name for grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f60eaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to do it: \n",
    "# groupby (filename, odk_type) combo\n",
    "# iterate over all groups\n",
    "# make a list of all pairwise combinations of nodes per group and iterate\n",
    "# contract each pair and see if you still have a DAG\n",
    "# Hint: in networkx nx.contracted_nodes(G,a,b) merges the nodes a and b to one new node CALLED 'a', \n",
    "# the attributes of 'b' end up as the 'contraction' attribute in 'a'\n",
    "# if yes, take the relevant of all the predecessor nodes of k\n",
    "# combine with OR and write it to all succesor nodes with AND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea406a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by duplicates\n",
    "dag2 = nx.DiGraph()\n",
    "while dag != dag2:    \n",
    "    g = df.groupby(['filename', 'odk_type'])\n",
    "    for i, frame in g: \n",
    "        if len(frame)>1:\n",
    "            nodes = list(frame.index)\n",
    "            for j, k in combinations(nodes, 2):\n",
    "                if j in dag.nodes() and k in dag.nodes():\n",
    "                    dag2 = nx.contracted_nodes(dag, j,k, self_loops=False)\n",
    "                    if nx.is_directed_acyclic_graph(dag2):\n",
    "                        # 1. Get the relevant of all predecessors of k\n",
    "                        r = ['(' + dag.nodes[j]['relevant'] + ')' for j in dag.predecessors(k) if dag.nodes[j]['relevant'] !='']\n",
    "                        if len(r)>0:\n",
    "                            for s in dag.successors(k):\n",
    "                                # 2. Append elements of 'r' joined with 'or' to all the successors 's' of 'k' with an 'and'\n",
    "                                dag.nodes[s]['relevant']= '(' + dag.nodes[s]['relevant'] + ')' + ' and (' + ' or '.join(r) + ')'\n",
    "                            # 3. replace dag by dag2\n",
    "                        dag = dag2\n",
    "                    else:\n",
    "                        print('In node', i, 'did not merge', j, 'and', k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef61d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a combined relevance attribute out of the original 'relevant' and the 'contraction' attribute\n",
    "[gt.make_node_relevant(dag, n) for n in dag.nodes if 'contraction' in dag.nodes[n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb1a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the max longest path from the original 'distance_from_root' and those from the contracted nodes\n",
    "[gt.make_node_distance(dag, n) for n in dag.nodes if 'contraction' in dag.nodes[n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf8c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the CDSS compatible topological sorting of the graph\n",
    "topo_order = gt.topo_sort_cdss_attrib(dag, 'distance_from_root') # the complete sorting of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d16c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.reindex(topo_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a935bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dea3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8de7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f5046a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a32da5a",
   "metadata": {},
   "source": [
    "### Sorting nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0171be0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a89474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. diagnosis and data_loader calculates can now be selected in a select_multiple. \n",
    "# If the right order given, the graph will be correctly sorted\n",
    "# for this to work we simulate a select_option ordering of the diagnosis\n",
    "# 2. To this we concat the other select_options of the TT graph (if existant)\n",
    "# -> we have a complete TT graph that can be sorted by the same functions as the DX graph\n",
    "a = linspace((len(diagnose_id_hierarchy)+1)/100, 0.01, num=len(diagnose_id_hierarchy)+1)\n",
    "d = dict(zip(diagnose_id_hierarchy, a))  # diagnosis priority \n",
    "opt_prio = gt.hierarchy_select_options(df_raw) # hierarchy of select_options in the form\n",
    "opt_prio = d | opt_prio # combine both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f5f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a graph entry point (typically a node pointing to the dataloader)\n",
    "rootelement = gt.get_graph_entry_point(dag, df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26468909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the CDSS compatible topological sorting of the graph\n",
    "topo_order = gt.get_topo_sort_cdss(dag, rootelement, opt_prio, df_raw) # the complete sorting of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afd02eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.reindex(topo_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb27c6ec",
   "metadata": {},
   "source": [
    "### Taking into account pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed773614",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The topological sorting does not take into account pages (page-containers). Objects that are on the same page, must be \n",
    "grouped in order to wrap them up in begin_group ... end_group in odk. The topological_sort does not know what. \n",
    "Therefore we resort df: all objects that belong the a page, get all ligned up below the page container, \n",
    "preserving their overall sorting in df.\n",
    "'''\n",
    "pageids = df.loc[df['odk_type']=='container_page'].index\n",
    "df.reset_index(drop=False, inplace=True)\n",
    "df['new_index']=pd.to_numeric(df.index)\n",
    "pagerows = df.loc[df['odk_type']=='container_page'].index\n",
    "df = df.merge(df[['id','new_index']], how='left', left_on='xml-parent', right_on='id', suffixes = ('', '_p'))\n",
    "df.loc[df['new_index_p'].notna(), 'new_index'] = df['new_index_p']\n",
    "df.drop(columns=['id_p','new_index_p'], inplace=True)\n",
    "\n",
    "g = df.groupby('xml-parent') # group by pages\n",
    "for name, frame in g:  # for each page\n",
    "    k=0.001\n",
    "    for i in frame.index: # for each element in that page\n",
    "        if df.loc[i,'xml-parent'] in pageids: # if we are in a real page and not in root\n",
    "            df.loc[i,'new_index'] = df.loc[i,'new_index']+k # add to the new index a small step\n",
    "            k+=0.001\n",
    "            \n",
    "df.set_index('new_index', drop=True, inplace = True)\n",
    "df.sort_index(inplace=True)\n",
    "df.set_index('id', drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e31e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 'end group' rows\n",
    "# get the last objects of each page\n",
    "df.reset_index(drop=False, inplace=True)\n",
    "index_endgrouprows = df.loc[~df.duplicated(subset='xml-parent', keep='last') & df['xml-parent'].isin(pageids)].index+0.1\n",
    "\n",
    "df_endgroup = pd.DataFrame(index=index_endgrouprows)\n",
    "df_endgroup['odk_type']='end group'\n",
    "df_endgroup['id']=df_endgroup.index\n",
    "\n",
    "df = pd.concat([df_endgroup, df])\n",
    "df.sort_index(inplace=True)\n",
    "df.set_index('id', drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d912cc",
   "metadata": {},
   "source": [
    "### update df via dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19ce67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making df look like the 'survey' tab in an xls form\n",
    "df[['repeat_count','appearance','required','required message::en','calculation']]=''\n",
    "df=df[['odk_type','name','value','help::en','hint::en','appearance','relevant','constraint', \\\n",
    "       'constraint_message','required','required message::en','calculation','repeat_count','image::en']]\n",
    "df.rename(columns={'odk_type':'type','value':'text','constraint_message':'constraint message::en'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e29d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {n:dag.nodes[n] for n in dag.nodes}\n",
    "dfa = pd.DataFrame.from_dict(d, orient='index')\n",
    "df.update(dfa)\n",
    "df.rename(columns={'text':'label::en', 'relevant':'relevance'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef90f0e4",
   "metadata": {},
   "source": [
    "### make df look according to xform standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e2bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# short term workaround for select_xxx + NAME to add the same name as list_name\n",
    "m = df['type'].isin(['select_one','select_multiple'])\n",
    "df.loc[m,'type'] = df.loc[m,'type'] + ' ' + df.loc[m,'name']\n",
    "\n",
    "# rename begin group\n",
    "df.replace({'container_page':'begin group'}, inplace=True)\n",
    "# add 'field-list'\n",
    "df.loc[df['type']=='begin group','appearance']='field-list'\n",
    "\n",
    "# in 'calculate' fields move 'relevance' to calculate\n",
    "df.loc[df['type']=='calculate','calculation'] = df.loc[df['type']=='calculate','relevance']\n",
    "# add 'number() to fit with odk '\n",
    "df.loc[df['type']=='calculate','calculation'] = 'number(' + df.loc[df['type']=='calculate','calculation'] + ')'\n",
    "# delete entry in relevance column of 'calcuate' rows\n",
    "df.loc[df['type']=='calculate','relevance'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdb261f",
   "metadata": {},
   "source": [
    "### make df_choices look according to xform standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac092d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making df_choices look like the 'choices' tab in an xls form\n",
    "df_choices.drop(columns=['odk_type'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f218059",
   "metadata": {},
   "source": [
    "### make a settings tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2e375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a 'settings' tab\n",
    "now = datetime.now()\n",
    "version=now.strftime('%Y%m%d%H%M')\n",
    "indx=[[1]]\n",
    "\n",
    "settings={'form_title':form_title,'form_id':form_id,'version':version,'default_language':'en','style':'pages'}\n",
    "df_settings=pd.DataFrame(settings,index=indx)\n",
    "df_settings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5cca95",
   "metadata": {},
   "source": [
    "### make a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c47596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import summary\n",
    "df_summary = summary.make_summary(df, df_choices, diagnose_id_hierarchy, summaryfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b58ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0bd825",
   "metadata": {},
   "source": [
    "### make constraint message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc250a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate constraint message to all select_multiple\n",
    "df.loc[df['type'].str.contains('select_multiple',na=False),'constraint']='.=\\'opt_none\\' or not(selected(.,\\'opt_none\\'))'\n",
    "df.loc[df['type'].str.contains('select_multiple',na=False),'constraint message::en']='**None** cannot be selected together with symptoms.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6679eef7",
   "metadata": {},
   "source": [
    "### load additional rows from external xls form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21dac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = loadcalc(df, drugsfile, form_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c49baf",
   "metadata": {},
   "source": [
    "### make a countdown timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05164507",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "From CHT Docs\n",
    "\n",
    "Countdown Timer: A visual timer widget that starts when tapped/clicked, and has an audible alert when done. \n",
    "To use it create a note field with an appearance set to countdown-timer. \n",
    "The duration of the timer is the field’s value, which can be set in the XLSForm’s default column. \n",
    "If this value is not set, the timer will be set to 60 seconds.\n",
    "\n",
    "Currently not implemented in TRICC, but hard coded here\n",
    "'''\n",
    "df.loc[df['label::en'].str.contains('START',na=False),'appearance']='countdown-timer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b5d48a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34294873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f66a36c",
   "metadata": {},
   "source": [
    "### Caretaker advice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0349c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ca.ca_expressions(df_raw, cafile)\n",
    "df = ca.update_ca_relevance(df, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7d627f",
   "metadata": {},
   "source": [
    "### add a diagnose message immediately after the diagnose (for MSFeCARE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424aae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if form_id!='almsom':\n",
    "    # show the detected diagnose right on detection\n",
    "\n",
    "    # read the diagnoses and the corresponding ids\n",
    "    df_diagnoses = pd.read_csv(diagnose_order)\n",
    "    diagnoses_dict=dict(zip(df_diagnoses.Name,df_diagnoses.id))\n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "    df.fillna('',inplace=True)\n",
    "    I = df.loc[df['name'].isin(diagnoses_dict.values())].index\n",
    "\n",
    "    for i in I:\n",
    "        d_message = pd.DataFrame({'index':df.loc[i]['index']+'_dm','type': 'note', \\\n",
    "                                    'name':'dm_' + df.loc[i]['name'],'label::en':\\\n",
    "                                    'Treatment for Diagnose: ' + df.loc[i]['label::en'],\\\n",
    "                                    'relevance':'number(${'+df.loc[i]['name']+'})=1'}, index=[i+0.1])\n",
    "\n",
    "        #df = df.append(d_message, ignore_index=False)\n",
    "        df = pd.concat([d_message, df], ignore_index=False)\n",
    "\n",
    "\n",
    "    # colorize the dm message\n",
    "    m = df['name'].str.contains('dm_',na=False)\n",
    "    df.loc[m,'label::en'] = '<span style=\"color: rgb(68, 28, 28);\">' + df.loc[m,'label::en'] + '</span>'\n",
    "\n",
    "    # sort rows and reset index\n",
    "    df = df.sort_index()\n",
    "    df.set_index('index',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d3c93",
   "metadata": {},
   "source": [
    "### Change appearance of help fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51465aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if form_id!='almsom':\n",
    "    from helpfields import helpfields\n",
    "    df = helpfields(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d58b99",
   "metadata": {},
   "source": [
    "### add required = true to all data entry fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3933bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[~df['type'].isin(['note','calculate','begin group','end group','text', 'acknowledge', '']) & (df['required']==''),'required']='true()'\n",
    "# but not to contextual parameters\n",
    "df.loc[df['name']=='data_load','required']=''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30899c5b",
   "metadata": {},
   "source": [
    "### combine multiple instances of a calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71fa30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probably no longer necessary calcultes should have been combined by node contraction\n",
    "# only those where node contraction would have created a loop remain, but there are probably none left\n",
    "df = calcombo(df, df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216d6b85",
   "metadata": {},
   "source": [
    "### handle duplicates in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c6d133",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.duplicated(subset=['name'],keep=False),'name']=df['name']+df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e5ac64",
   "metadata": {},
   "source": [
    "### write xls form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa3b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a Pandas Excel writer using XlsxWriter as the engine\n",
    "writer = pd.ExcelWriter(ttfile, engine='xlsxwriter')\n",
    "\n",
    "\n",
    "df.to_excel(writer, sheet_name='survey',index=False)\n",
    "df_choices.to_excel(writer, sheet_name='choices',index=False)\n",
    "df_settings.to_excel(writer, sheet_name='settings',index=False)\n",
    "\n",
    "#close the Pandas Excel writer and output the Excel file\n",
    "writer.save()\n",
    "\n",
    "# run this on a windows python instance because if not then the generated xlsx file remains open\n",
    "writer.close()\n",
    "writer.handles = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d94176f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102ddec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's left to be done: \n",
    "# 1. if TT is to be merged with dx: handle the dataloader -> see if this is not done at the merge script level\n",
    "# 2. Put data_load into a group\n",
    "\n",
    "# 4. for somalia: integrate html files\n",
    "# 5. for msf: contract nodes based on 'name' not on 'text' -> adapt drawing accordingly\n",
    "# 6. at the very end: handle duplicate names\n",
    "\n",
    "# 8. for alm som: make the CA advice, and get rid of the 'come back' messages\n",
    "# 9. Write output into a logfile, rather then into the jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a12254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871a5b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5909e099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b34c57d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e82db3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5044b641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee268671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for almanach somalia only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfddb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the global flow\n",
    "from formconverters import df2xlsform # makes xlsforms out of dataframes\n",
    "df2xlsform(df, df_choices, df_settings, '/home/rafael/Documents/git/cht-core/config/ecare/forms/app/almsom.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "(cd /home/rafael/Documents/git/cht-core/config/ecare/ && cht --url=https://medic:password@localhost --accept-self-signed-certs convert-app-forms upload-app-forms -- almsom)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
